%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{\emph{Consort}: a model of composition}
\label{chap:a-model-of-composition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
<abjad>[hide=true]
import collections
import consort
</abjad>
\end{comment}

\begin{comment}
\begin{markdown}
-   Materials
-   Configuration
-   Templating
-   Layers
-   Coarse to fine
-   Rhythm first
-   What is specification? What is a specifier? What is configuration and
    aggregation?
-   What should happen musically, where should it happen?
-   What is material?
-   What is music?
-   Rhythm is interpreted first, as all other parameters depend on it.
-   Rhythm is interpreted from "coarse" to "fine": from the level of phrase
    boundaries to the level of individual notes, rests, tuplets and ties.
-   This discussion only focuses on notation, nothing related to aesthetic
    experience, physical modeling or anything else. This is a tool for a
    specific composer to create scores, not a discussion explicitly of why they
    would work this way (although that should be discussed in the conclusion).
-   Specification and interpretation conceive of the score as a single, hugely
    complex expression.
-   Templating as variation.
-   Define what composition means here: laying out symbols on the page.
-   This way of thinking and working does not attempt to define or even model
    concepts like "melody" or even "phrase". They're too vague. If we use that
    terminology at all, it is only in the most incredibly constrained way.
\end{markdown}
\end{comment}

Consort, a Python library written as an extension to Abjad, models the
composition of notated musical score as a repeated cycle containing three
distinct stages: \emph{specification}, \emph{interpretation} and
\emph{visualization}.

What follows is a detailed analysis of the various algorithms and subroutines
employed during Consort's specification and interpretation stages.

\section{Specification}
\label{sec:specification}

\emph{Specification} describes how \emph{out-of-time materials} -- both
concrete and programmatic -- should be deployed \emph{in-time} in a
\emph{segment} of musical score as notation. Materials encompass abstractions
-- such as pitch sets or collections of performance technique indications --,
concrete fragments of \emph{music} -- narrowly defined here as any contiguous
selection of score components --, and procedures for producing, altering or
embellishing music such as rhythm-makers or attachment-handlers. Score segments
comprise any contiguous passage of music, demarcating an area of compositional
concern. Consort treats scores as comprised of at least one segment, but
potentially many more concatenated together. Any segment may of course contain
arbitrarily complex inner structuring. Separation of scores into distinct
segments acts then mainly as an aid for the composer, both by simplifying the
complexity of the current specification under consideration, and by allowing
the typesetting engine -- LilyPond -- to display more manageable amounts of
notation than the full score, thus speeding up the cycle of specifying,
interpreting and visualizing.

\subsection{Segment-makers}
\label{ssec:segment-makers}

Composers specify segments by creating and progressively configuring
\emph{segment-makers}, classes which conceptually mirror the rhythm- and
timespan-makers described in \autoref{chap:time-tools}, but on a much larger
scale. Such configuration parameters include tempo, permitted meters, desired
duration, and score template. Score templates, as outlined in
\autoref{ssec:score-templates}, are notation factories which build scores
comprised of staff groups, staves, voices, clefs and instruments, as necessary
to model the context hierarchy of a score to which no count-time components
have yet been added. All segment-makers in a score project must use the same
score template if they are to appear contiguously in the complete typeset
score. LilyPond will automatically concatenate scores with identical context
hierarchies. All contexts need to be present in every concatenated segment,
otherwise LilyPond will concatenate incorrectly. However, various typographic
overrides can be employed to make it seem that a context has disappeared.

The following defines a segment-maker with desired duration of 9 seconds, only
3/4 meters permitted, a score-template comprised of two rhythmic staves, and a
tempo or quarter-equals-60:

\begin{comment}
<abjad>
segment_maker = consort.SegmentMaker(
    desired_duration_in_seconds=9,
    permitted_time_signatures=[(3, 4)],
    score_template=templatetools.GroupedRhythmicStavesScoreTemplate(
        staff_count=2,
        with_clefs=True,
        ),
    tempo=indicatortools.Tempo((1, 4), 60),
    )
</abjad>
\end{comment}

\noindent This segment-maker can be illustrated via \texttt{show()}, like many
other objects in Abjad. Illustration here invokes the segment-maker's
interpretation stage. The \texttt{verbose=False} flag prevents it from
printing a considerable amount of diagnostic information:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\noindent By changing the tempo from quarter-equals-60 to quarter-equals-20,
the overall notated duration of the segment shrinks by two thirds, but the
duration in seconds remains the same. This mechanism allows segments to be
planned relative one another in terms of their \enquote{actual} durations:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
slower_segment_maker = new(
    segment_maker,
    tempo=indicatortools.Tempo((1, 4), 20),
    )
show(slower_segment_maker, verbose=False)
</abjad>
\end{comment}

\noindent Most importantly, segment-makers may be configured with any number of
\emph{music settings}, which aggregate a timespan-maker, a target timespan and
any number of \emph{music specifiers} -- bundles of materials which describe
how phrases of music should be produced within a single voice.

\subsection{Music settings}
\label{ssec:music-settings}

\begin{markdown}
-   What kind of music, laid out in what pattern, in which general
    time frame, for which voices, in what layer?
-   A timespan-maker
-   A target timespan
    -   A timespan.
    -   A timespan inventory.
    -   A ratio/parts expression.
        -   Demonstrate. Check.
-   Voice abbreviation / music specifier pairs
    -   Why abbreviate? It's because of Python's key=value syntax for
        keyword arguments.
    -   How do we convert abbreviation to an actual context name?
        -   Score templates need to define a mapping.
        -   Demonstrate. Check.
    -   What is the context name for?
        -   Name-wise indexing in an actual score.
        -   Demonstrate. Check.
    -   Also, composite music specifiers: when two contexts need to be
        discussed as a single unit.
        -   Composite music specifiers need to convert one abbreviation into
            two more abbreviations, each mapped to an actual context name.
        -   Consort's ScoreTemplateManager takes care of calculating and
            caching the appropriate names and abbreviations.
-   Layer is implicit, derived from the order of setting definitions.
    -   Recall the discussion of timespan layer from earlier chapters.
\end{markdown}

Music settings object-model both \emph{when} and in
\emph{which} voices musical materials should be deployed. The order in which
composers configure segment-makers with music settings defines each music
setting's \emph{layer} -- the first setting defined being layer 0, the second
layer 1 and so forth, with each higher layer number indicating higher
precedence or \enquote{foregroundness} --, determining how overlapping events
in a single voice will mask one another. Score materials, including music
settings, music specifiers, timespan-makers and any other class pertinent to
score creation -- potentially even other segment-makers, may be defined from
scratch in the same code module as the segment-maker currently being
configured, templated from another material, or simply imported into the
segment definition's namespace.

\subsection{Music specifiers}
\label{ssec:music-specifiers}

Music specifiers bundle all of the information necessary for a segment-maker to
generate notation for a sequence of divisions, grouped as a phrase.

a rhythm-maker, grace-handler, pitch-handler and attachment-handler, as well as
a variety of other properties.

As demonstrated in \autoref{ssec:dependent-timespan-makers}, dependent
timespan-makers can be configured to create timespans according to the
disposition of performed timespans associated with specific voices.

Dependent timespan-makers can also be configured to select depended-upon
timespans based on the \texttt{label} property of those performed timespans'
music specifiers.

This helps model the situation of creating a pedal voice mirroring not simply a
pianist's left- and right-hand events, but only those that actually depress
keys, ignoring guero events or other off-the-key percussive techniques for
which no pedaling is desired.

\begin{comment}
<abjad>
unlabeled_music_specifier = consort.MusicSpecifier()
labeled_music_specifier = consort.MusicSpecifier(labels=['labeled'])
timespan_inventory = timespantools.TimespanInventory([
    consort.PerformedTimespan(
        layer=1,
        start_offset=0,
        stop_offset=4,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=2,
        stop_offset=7,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 2',
        ),
    consort.PerformedTimespan(
        layer=2,
        start_offset=6,
        stop_offset=8,
        music_specifier=unlabeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=2,
        start_offset=10,
        stop_offset=(25, 2),
        music_specifier=unlabeled_music_specifier,
        voice_name='Voice 2',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=11,
        stop_offset=14,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=14,
        stop_offset=16,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=15,
        stop_offset=16,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 2',
        ),
    ])
show(timespan_inventory, key='voice_name')
</abjad>
\end{comment}

\begin{comment}
<abjad>
dependent_timespan_maker = consort.DependentTimespanMaker(
    include_inner_starts=True,
    voice_names=('Voice 1', 'Voice 2'),
    )
result = dependent_timespan_maker(
    layer=3,
    music_specifiers={'Voice 3': None},
    timespan_inventory=timespan_inventory[:],
    )
show(result, key='voice_name')
</abjad>
\end{comment}

\begin{comment}
<abjad>
dependent_timespan_maker = new(
    dependent_timespan_maker,
    labels=['labeled'],
    )
result = dependent_timespan_maker(
    layer=3,
    music_specifiers={'Voice 3': None},
    timespan_inventory=timespan_inventory[:],
    )
show(result, key='voice_name')
</abjad>
\end{comment}

\begin{markdown}
-   A bundle of descriptors for what kind of music should fill a series of
    1 or more timespans.
-   All of the descriptors are optional.
-   Rhythm-makers
-   Grace-handlers
-   Pitch-handlers
-   Attachment-handlers
-   Labels
    -   This works in tandem with DependentTimespanMaker.
    -   Example? Piano music with two hands and pedaling. The
        hand-performed music might involve key presses, or it might involve
        percussive techniques. All of the techniques that require pedaling
        should be labeled 'pedaled'. A DependentTimespanMaker for the pedal
        context can then be configured to look at timespans for both the LH
        and RH of the piano, but only those timespans configured with a
        music specifier labeled 'pedaled'.
    -   Demonstrate.
-   minimum phrase duration
    -   (should this be hoisted into PerformedTimespan?
-   seed
-   Composite music specifiers
\end{markdown}

A music specifier's \texttt{seed} property

A music specifier's \texttt{minimum\_phrase\_duration} property

\section{Rhythmic interpretation}
\label{sec:rhythmic-interpretation}

At any point during specification, a segment-maker may be interpreted to
produce an illustration. Score interpretation proceeds conceptually much like
compilation in classical computing, where a compiler parses an instruction set
written in some source language into an intermediate representation and then
transforms that same intermediate representation into instructions executable
on a target platform. In Consort's interpretation stage, the compiler is the
segment-maker itself, and the source instruction set its configuration -- its
tempo, permitted meters, music settings and so forth. Timespan inventories
produced by each music setting's timespan-maker, populated with timespans
annotated with music specifiers perform the role of the intermediate
representation. This intermediate representation acts as a \emph{maquette},
blocking out where in the resulting score segment various materials should be
deployed. The target of score interpretation is, unsurprisingly, a fully-fledge
score aggregated from Abjad score components. Interpretation takes place in two
broad stages -- rhythmic interpretation, followed by non-rhythmic
interpretation -- with the first stage producing a score populated solely with
rhythmic information, and the second stage applying grace notes, pitches,
indicators, spanners and various typographic overrides to the
previously-constructed rhythmic skeleton.

Seen from a high level, rhythmic interpretation proceeds from coarse- to
fine-grained. The segment-maker creates a *maquette* -- a model of the locations
of musical materials in the score -- by calling each of its
music-settings in turn to populate a timespan inventory. It then resolves
overlap conflicts within that inventory, fits meters against the resolved
inventory's offsets, splits and prunes the contents of the inventory according
to its fitted metrical structure, and finally converts the finished timespan
maquette into an actual score. This process, like interpretation overall, can
be roughly divided into work flows of \emph{maquette creation} and \emph{music
creation}, although in practice the two flows are interleaved significantly as
they actually influence one another. When creating the maquette, music settings
with \emph{independent} timespan-makers -- those which do not depend on the
contents of a previously created timespan inventory, specifically flooded and
talea timespan-makers -- are called in a first pass, and those with {dependent}
timespan-makers in a second. These two passes only differ significantly in that
meters are fitted against the segment's timespan maquette during the
independent timespan-maker pass, but not during the dependent.

\subsection{Populating the maquette}
\label{ssec:populating-the-maquette}

To populate the maquette, the segment-maker calls each of its music settings to
produce timespans according to their configured timespan-makers,
\emph{timespan-identifiers} -- optional specifications of which portion of the
segment's overall timespan to operate within -- and voice-associated music
specifiers. Timespan identifiers may include timespans, inventories of
timespans, or even expressions callable against the segment-makers own timespan
which evaluate to an inventory of timespans.

Music settings exist without any reference to a segment-maker, its desired
duration -- and therefore desired timespan --, or its score template. In order
to know which target timespan or timespans a music setting's timespan-maker
should operate within -- in the case of procedural timespan identifiers such as
\emph{ratio-parts expressions} which must be called against a preexisting
timespan in order to determine what part or parts of that timespan to use --
the music setting must resolve its timespan identifier against the segment's
desired duration. Target timespan resolution must also take into account offset
quantization, as the target timespans resulting from the evaluation of a
ratio-parts expression may not align against a power-of-two-denominator offset
grid such as 1/8, 1/16 or 1/32. Because timespan-makers produce their output
relative to the start-offset of their target timespan, a misaligned target
timespan -- starting at an offset like 1/3 or 15/7 rather than 1/4 or 0 -- will
cause all generated timespans to be misaligned.

Music setting's associate their music specifiers with strings containing
voice-name abbreviations. These abbreviations are always underscore-delimited
strings such as \texttt{violin\_1} or \texttt{piano\_lh} -- necessitated by
Python's keyword argument syntax so that they can be used as keys during class
instantiation -- which represent voices in a score, without having established
a concrete reference to those voice contexts. In order to match its music
specifiers against actual voice contexts in a score, the music setting must
resolve its voice-name abbreviations against a score template, looking up each
abbreviation on the template and returning the real name of the associated
context. This lookup process allows music settings to construct well-formed
voice-name-to-music-specifier mappings, implemented as ordered dictionaries and
ordered by the actual *score index* -- effectively, the vertical location -- of
each looked-up context in the segment-maker's under-construction score. As
demonstrated in \autoref{sec:timespan-makers}, timespan-makers require these
mappings to produce their output. Additionally, voice-name resolution
guarantees that the values in the resolved voice-name-to-music-specifier
mapping are always either a \texttt{MusicSpecifierSequence} or
\texttt{CompositeMusicSpecifier} instance via coercion, where any composite
music-specifier's primary and secondary music specifiers are themselves coerced
into music specifier sequences. This coercion ensures that all arguments to the
music setting's timespan-maker are in a well-formed and predictable state.
\footnote{Timespan-makers actually delegate the creation of performed and
silent timespans to music specifier sequences. While not demonstrated
explicitly in \autoref{sec:timespan-makers}, this delegation allows
timespan-makers to use both music specifier sequences and composite music
specifiers interchangeably, with the former creating timespans associated with
one voice and the later with two. When the values of a timespan-maker's input
voice-name-to-music-specifier mapping are neither music specifier sequences nor
composite music specifiers -- as was the case in all of the examples in
\autoref{sec:timespan-makers} -- they implicitly coerce those values into music
specifier sequences.}

\begin{comment}
<abjad>
music_setting = consort.MusicSetting(
    timespan_identifier=consort.RatioPartsExpression(
        parts=(0, 2),
        ratio=(1, 3, 2),
        ),
    timespan_maker=consort.FloodedTimespanMaker(),
    violin_2_lh='A',
    viola_lh=('B', 'C', 'D'),
    )
</abjad>
\end{comment}

\begin{comment}
<abjad>
score_template = consort.StringQuartetScoreTemplate()
result = music_setting.resolve_music_specifiers(score_template)
for voice_name, music_specifier_sequence in result.items():
    print('VOICE:', voice_name)
    print(format(music_specifier_sequence))

</abjad>
\end{comment}

\begin{comment}
<abjad>
segment_timespan = timespantools.Timespan(0, 8)
show(segment_timespan)
target_timespans = music_setting.resolve_target_timespans(segment_timespan)
show(target_timespans, range_=(0, 8))
</abjad>
\end{comment}

\begin{comment}
<abjad>
target_timespans = music_setting.resolve_target_timespans(
    segment_timespan,
    timespan_quantization=Duration(1, 16),
    )
show(target_timespans, range_=(0, 8))
</abjad>
\end{comment}

\begin{comment}
<abjad>
music_setting = new(
    music_setting,
    timespan_identifier__mask_timespan=timespantools.Timespan(
        start_offset=(1, 2),
        stop_offset=7,
        ),
    )
target_timespans = music_setting.resolve_target_timespans(segment_timespan)
show(target_timespans, range_=(0, 8))
</abjad>
\end{comment}

Once resolved, each music setting can call its timespan-maker to create
timespans with the appropriate voice-name-to-music-specifier mapping, target
timespans and layer, adding the contents of the resulting inventory to the
growing maquette of performed and silent timespans produced by previous music
settings. The populating process repeats until no more music settings remain.

\subsection{Resolving cascading overlap}
\label{ssec:resolving-cascading-overlap}

\begin{comment}
<abjad>
layer_1_timespan_maker = consort.FloodedTimespanMaker()
layer_1_target_timespan = timespantools.Timespan(0, (19, 4))
layer_1_music_specifiers = collections.OrderedDict([
    ('Voice 2', None),
    ('Voice 3', None),
    ])
layer_1 = layer_1_timespan_maker(
    layer=1,
    music_specifiers=layer_1_music_specifiers,
    target_timespan=layer_1_target_timespan,
    )
show(layer_1, key='voice_name', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{comment}
<abjad>
layer_2_timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea(
        counts=(0, 1, 3),
        denominator=8,
        ),
    playing_groupings=(1, 2),
    playing_talea=rhythmmakertools.Talea(
        counts=(1, 2, 3, 4),
        denominator=4,
        ),
    silence_talea=rhythmmakertools.Talea(
        counts=(5, 3, 1),
        denominator=8,
        ),
    )
layer_2_target_timespan = timespantools.Timespan((3, 4), (19, 4))
layer_2_music_specifiers = collections.OrderedDict([
    ('Voice 1', None),
    ('Voice 2', None),
    ('Voice 3', None),
    ('Voice 4', None),
    ])
layer_2 = layer_2_timespan_maker(
    layer=2,
    music_specifiers=layer_2_music_specifiers,
    target_timespan=layer_2_target_timespan,
    )
show(layer_2, key='voice_name', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{comment}
<abjad>
layer_3_timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea(
        counts=(0, 0, 0, 1),
        denominator=8,
        ),
    padding=Duration(1, 4),
    playing_talea=rhythmmakertools.Talea(
        counts=(2, 3, 4),
        denominator=8,
        ),
    silence_talea=rhythmmakertools.Talea(
        counts=(6,),
        denominator=4,
        ),
    synchronize_step=True,
    )
layer_3_target_timespan = timespantools.Timespan((6, 4), (21, 4))
layer_3_music_specifiers = collections.OrderedDict([
    ('Voice 1', None),
    ('Voice 3', None),
    ('Voice 4', None),
    ])
layer_3 = layer_3_timespan_maker(
    layer=3,
    music_specifiers=layer_3_music_specifiers,
    target_timespan=layer_3_target_timespan,
    )
show(layer_3, key='voice_name', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{comment}
<abjad>
timespan_makers = (
    layer_1_timespan_maker,
    layer_2_timespan_maker,
    layer_3_timespan_maker,
    )
music_specifiers = (
    layer_1_music_specifiers,
    layer_2_music_specifiers,
    layer_3_music_specifiers,
    )
target_timespans = (
    layer_1_target_timespan,
    layer_2_target_timespan,
    layer_3_target_timespan,
    )
triples = zip(timespan_makers, music_specifiers, target_timespans)
timespan_inventory = timespantools.TimespanInventory()
for layer, triple in enumerate(triples, 1):
    timespan_inventory = triple[0](
        layer=layer,
        music_specifiers=triple[1],
        target_timespan=triple[2],
        timespan_inventory=timespan_inventory,
        )

show(timespan_inventory, key='voice_name', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{comment}
<abjad>
show(timespan_inventory, range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{comment}
<abjad>
demultiplexed_timespans = consort.SegmentMaker.demultiplex_timespans(
    timespan_inventory,
    )
show(demultiplexed_timespans, range_=(0, (21, 4)))
</abjad>
\end{comment}

\subsection{Finding meters, revisited}
\label{ssec:finding-meters-revisited}

Consort's segment-maker implements a variation on the meter-fitting algorithm
described in \autoref{sec:finding-meters}. Each segment-maker may be configured
with an inventory of permitted meters, as well as maximum meter run length, in
order to drive the meter fitting algorithm. When counting offsets,
segment-makers include the offsets found on the performed timespans in their
maquette but discard those from silent timespans, removing any influence from
timespans created solely for silencing other timespans. The start offset of
each performed timespan is weighed twice as much as their stop offset. This
imbalance helps emphasize simultaneous phrase starts across different voices.
Additionally, segment-maker's weight their own desired stop offset at a much
higher value than any count derived from the offsets in their maquette. This
attempts to influence the meter fitting process into selecting a series of
meters which end as close to their desired stop offset as possible. After
fitting meters, the segment-maker caches both the fitted meters and their
boundaries as properties on its instance, affording later retrieval by other
subroutines.

\subsection{Splitting, pruning \& consolidation}
\label{ssec:splitting-pruning-and-consolidation}

Once meters have been fitted against the resolved maquette, the timespans in
the maquette must be split at the measure boundaries outlined by those meters.
Splitting guarantees that no timespans cross any bar-lines and that therefore
no containers generated by those timespans when notating them as score
components cross any bar-lines either. While LilyPond can typeset bar-line
crossing notes, chords and even tuplets, the scores I have composed via Consort
do not currently make use of such constructions. As described in
\autoref{ssec:operations-on-timespans}, operations on timespans which change
offsets -- generating new timespans in the process, rather than modifying the
operated-upon timespan in-place -- preserve their unmodified properties via
templating. Splitting is no exception, and those timespans split maintain their
music specifiers, layer identifiers and voice-names:

\begin{comment}
<abjad>
performed_timespan = consort.PerformedTimespan(
    layer=3,
    start_offset=(1, 2),
    stop_offset=(13, 8),
    voice_name='Percussion Voice',
    )
shards = performed_timespan.split_at_offset((9, 16))
print(format(shards))
</abjad>
\end{comment}

\noindent After splitting, the segment-maker prunes timespans considered either
too short or malformed. Performed timespans may be configured with a
\texttt{minimum\_duration} property. Timespan-makers may set this property on
timespans they create when they are themselves configured with a
\texttt{TimespanSpecifier}. Any performed timespan whose actual duration is
less than its minimum duration -- if it has been configured with a minimum
duration -- will be removed from the maquette. Likewise any timespan with a
duration of 0 -- therefore malformed -- will also be removed. While the latter
pruning guarantees correctness of the maquette -- malformed timespans cannot be
rendered as notation at all, and may cause other problems when partitioning due
to ambiguities in their start / stop offset semantics --, the former allows for
a kind of compositional control over the maquette. When notated with certain
rhythm-makers, overly short divisions -- especially those shorter than
1/8-duration -- may give undesirable results. Note that silent timespans have
no configurable minimum duration. Their \texttt{minimum\_duration} always
returns 0. They maintain this dummy property so that the segment-maker's
timespan-pruning algorithms can treat silent and performed timespans
identically.

Next, Consort's segment-maker \emph{consolidates} contiguous performed
timespans with identical music specifiers, caching the durations of the
consolidated timespans in a new timespan's \texttt{divisions} property. Each
new consolidated timespan outlines the start and stop offset of its
consolidated group:

\begin{comment}
<abjad>
timespans = timespantools.TimespanInventory([
    consort.PerformedTimespan(
        start_offset=0,
        stop_offset=10,
        music_specifier='foo',
        ),
    consort.PerformedTimespan(
        start_offset=10,
        stop_offset=20,
        music_specifier='foo',
        ),
    consort.PerformedTimespan(
        start_offset=20,
        stop_offset=25,
        music_specifier='bar',
        ),
    consort.PerformedTimespan(
        start_offset=40,
        stop_offset=50,
        music_specifier='bar',
        ),
    consort.PerformedTimespan(
        start_offset=50,
        stop_offset=58,
        music_specifier='bar',
        ),
    ])
show(timespans)
timespans = consort.SegmentMaker.consolidate_timespans(timespans)
show(timespans)
</abjad>
\end{comment}

\noindent Consolidation transforms performed timespans from free-floating cells
in the maquette into components of larger phrases. The cached divisions also
prepare these consolidated timespans for \emph{inscription} by defining the
correct input for a rhythm-maker: a sequence of divisions. If the music
specifier of the consolidated timespan was configured with a \emph{minimum
phrase duration}, and the consolidated timespan falls under that threshold, it
too is discarded.

\subsection{Inscription}
\label{ssec:inscription}

\begin{markdown}
-   Divisions
-   Get rhythm maker (under what circumstances?)
-   Make simple music
-   Consolidate rests
-   Rhythmic post-processing
    -   attaching a GeneralizedBeam
    -   attaching the scoped music specifier
-   Reconfigure the timespan with the created music, but without the divisions
\end{markdown}

\begin{comment}
<abjad>
parseable = r'''
\new Voice {
    {
        { \time 4/4 r4 c4 }
        { \times 2/3 { c4 r8 } r4 }
        { \time 4/4 r4 c4. r8 }
        { r4 \break }
        { \time 3/4 r4 }
        { c8 c4. }
        \times 2/3 { \time 2/4 r4 c4 c4 }
        { \time 4/4 c4. r8 }
        { r8 c4 r8 }
    }
}
'''
unconsolidated_staff = Staff(parseable, context_name='RhythmicStaff')
consort.annotate(unconsolidated_staff)
show(unconsolidated_staff)
</abjad>
\end{comment}

\begin{comment}
<abjad>
consolidated_staff = Staff(parseable, context_name='RhythmicStaff')
for voice in consolidated_staff:
    for phrase in voice:
        phrase = consort.SegmentMaker.consolidate_rests(phrase)

consort.annotate(consolidated_staff)
staff_group = StaffGroup([unconsolidated_staff, consolidated_staff])
show(staff_group)
</abjad>
\end{comment}

\subsection{Meter pruning}
\label{ssec:meter-pruning}

After the timespan pruning outlined in
\autoref{ssec:splitting-pruning-and-consolidation}, and the possibility of gaps
introduced due to rest consolidation as outlined in \autoref{ssec:inscription},
the overall stop offset of the maquette -- not the stop offset derived from the
segment-maker's desired duration -- may have shifted earlier. Depending on the
degree of shift, timespans in the maquette may no longer occur during one or
more of the implicit timespans of the previously fitted meters. Segment-makers
may be configured to discard these silences via their
\texttt{discard\_final\_silence} property, progressively removing meters from
the end of the list of fitted meters until one overlaps at least one performed
timespan in the maquette.

\subsection{Populating dependent timespans}
\label{ssec:populating-dependent-timespans}

The previous few passages, from \autoref{ssec:populating-the-maquette} through
\autoref*{ssec:meter-pruning}, describe the process of populating a
segment-makers's timespan maquette with the products of its *independent* music
settings -- those music settings whose timespan makers are independent, notably
flooded and talea timespan-makers.

With the maquette partially populated, those music settings with dependent
timespan-makers -- timespan-makers which generate timespans based on the
contents of a preexisting timespan inventory -- may finally be called to
provide their contributions to the maquette.

Dependent population proceeds almost identically to independent population with
a few notable differences.

\begin{markdown}
-   this occurs almost identically as with independent timespans,
    but without any meter finding
-   populate demultiplexed timespans
-   demultiplex
-   split at meter boundaries
-   consolidate
-   inscribe
\end{markdown}

\subsection{Populating silent timespans}
\label{ssec:populating-silent-timespans}

With the maquette finally populated by inscribed performed timespans, properly
split, resolved and consolidated, the segment-maker can fill in the remaining
gaps between phrases in each voice. This is accomplished by creating
rest-inscribed timespans for each gap. As there are no more layers to add to
the maquette, the segment-maker can populate, split and inscribe timespans for
each of these gaps in a single pass. For each voice in the segment-maker's
still-empty score, the segment-maker retrieves all timespans -- if any --
associated with that voice and subtracts each of them in turn from a single
silent timespan the length of the entire segment, as determined by the first
and last meter boundaries. Any remaining shards from that segment-length silent
timespan represent gaps between phrases in that voice. If the maquette contains
no performed timespans associated with that voice, the segment-length silent
timespan remains unaltered. The segment-maker then splits the remaining silent
timespan shards at every intersecting meter boundary, as described in
\autoref{ssec:splitting-pruning-and-consolidation}, guaranteeing that the
resulting silent timespans do not cross bar-lines. Once split, the
segment-maker partitions the silent timespans, and iterates over the
partitioned groups. For each group of contiguous silent timespans, it generates
a phrase of music containing only rests using a fully-masked note rhythm-maker
-- as described in \autoref{ssec:inscription} --, attaches a dummy
music-specifier to the phrase and instantiates a performed timespan annotated
with that phrase, adding it to the current voice's timespan inventory in the
demultiplex maquette.

\subsection{Rewriting meters, revisited}
\label{ssec:rewriting-meters-revisited}

Once its maquette is completely populated, Consort's segment-maker performs
meter rewriting. This proceeds in a more elaborate manner than the meter
rewriting process as outlined in \autoref{sec:rewriting-meters} and
\autoref{ssec:rewriting-meters}, and involves a number of notable differences.

For reasons of computational efficiency, Consort rewrites the meters in
each phrase of music annotating each performed timespan in the maquette before
those phrases have even been inserted into the segment-maker's score. Meter
rewriting involves potentially many alterations to the contents of containers
due to fusing and splitting, as well as many duration and offset lookups.
Anytime a component is replaced or its duration changed, the cached offsets of
components located in the score tree after the changed component as well as
the durations of its parents are invalidated. They must be recomputed and
re-cached on the next offset lookup performed on any component the score tree.
Delaying inserting each inscribed timespan's music into the segment-maker's
score guarantees that that music's score depth remains shallow, and therefore
limits the complexity of offset calculation during rewriting.

When beginning the meter rewriting process, the segment-maker converts its
fitted meters into a timespan collection -- Consort's optimized timespan
inventory class -- containing timespans annotated with those fitted meters, one
per timespan.

\begin{comment}
<abjad>
meters = metertools.MeterInventory([(3, 4), (2, 4), (6, 8), (5, 16)])
meter_timespans = consort.SegmentMaker.meters_to_timespans(meters)
print(format(meter_timespans))
</abjad>
\end{comment}

\noindent Representing meters as timespans provides two important benefits.
First, meters intersecting a given division within a phrase can be efficiently
located using the search methods implemented on \texttt{TimespanCollection}.
Second, the time relation methods implemented on \texttt{Timespan} can be used
to test if a given division's timespan is congruent -- that is, possesses an
identical start and stop offset -- to a meter's timespan. Divisions containing
solely rests which are also congruent to a meter timespan can be rewritten with
full-bar rests.

The segment-maker then proceeds through each demultiplexed timespan inventory
in the maquette, iterating over each timespan, then over each division in that
performed timespan's music. Timespans whose rhythm-maker forbids meter
rewriting -- via the \texttt{forbid\_meter\_rewriting} flag on an optional
\texttt{DurationSpellingSpecifier} -- are skipped over.\footnote{It may be
undesirable to rewrite a rhythm's meter in certain situations, particularly if
a composer is trying to avoid the introduction of ties or dots for whatever
reason.} In order to determine which meter governs a division, that division's
timespan must be retrieved and then translated before it can be used to search
the inventory of meter timespans. Because each phrase of music annotating each
performed timespan has not yet been inserted into the score, they all express
their start offset as 0. Likewise, each phrase's child divisions express their
start offsets relative to their parent phrase's 0 start offset. Translating
each division's timespan relative to the start offset of the performed timespan
annotated by that division's phrase provides a useful search target for the
meter timespan inventory. The translated division timespan represents the
timespan that division \emph{would} occupy if its phrase, and all other
phrases, had already been inserted into the appropriate voice in the score.
Intersecting meters can then be found through a simple search and retranslated
relative to the current performed timespan's start offset, giving their
appropriate location within the not-yet-inserted phrase. The following
operations outline the translation and search process:

\begin{comment}
<abjad>
inscribed_timespan = consort.PerformedTimespan(
    start_offset=(5, 4),
    stop_offset=(9, 5),
    music=Container("{ c'4 }{ c'2 }{ c'4 }"),
    )
division = inscribed_timespan.music[1]
division_timespan = inspect_(division).get_timespan()
print(format(division_timespan))
translation = inscribed_timespan.start_offset
division_timespan = division_timespan.translate(translation)
print(format(division_timespan))
meter_timespan = meter_timespans.find_timespans_intersecting_timespan(
    division_timespan)[0]
print(format(meter_timespan))
translation = -1 * division_timespan.start_offset
meter_timespan = meter_timespan.translate(translation)
print(format(meter_timespan))
</abjad>
\end{comment}

\noindent With the appropriate meters selected, rewriting continues very much
as described in \autoref{ssec:rewriting-meters}. Tuplets are rewritten solely
with respect for the pre-prolated contents durations, and unprolated containers
are rewritten with respect for their intersecting meter, with an initial
offsets applied to the meter rewriting process if they happen to start later
than their meter. Additionally, Consort's meter rewriting tests silent meters
-- those whose leaves consist entirely of rests -- for congruency with the
current meter. Any division consisting solely of rests which also begins and
ends at the start and stop offsets of a meter's timespan can be rewritten
instead as a single full-bar rest. The segment-maker also attaches a
\texttt{StaffLinesSpanner} to the silent division, which collapses the staff
down to a single line, giving the score a fragmented appearance. Finally,
Consort's segment-maker performs a logical-tie cleanup pass, fusing all
2-length logical ties consisting of matched pairs of 1/16 or 1/32 notes. This
takes care of some possible artifacts of heavily subdivided meter-rewriting,
and makes the final rhythmic output generally more readable.

\subsection{Populating the score}
\label{ssec:populating-the-score}

After meter-rewriting, the segment-maker can finally populate its score. To do
so, it iterates through its timespan maquette and still-unpopulated score in
parallel, extracting the inscribed music from each performed timespan in the
maquette and inserting those phrases into the score in the appropriate voice.

\section{Non-rhythmic interpretation}

\subsection{Score traversal}

The various handlers which iterate through the score during non-rhythmic
interpretation use one of two techniques.

*Voice-wise* iterations iterates through all voice contexts in the score, then
iterates through the top-level containers in those voices. These top-level
containers are the same containers reference by each performed timespan's
\texttt{music} property, and represent each phrase in the maquette.

*Attack-point* iteration iterates through all logical ties in the score in
*time order* according to their start offset in the score, regardless of their
vertical position within the score. Logical ties with identical start offsets
-- those appearing at simultaneities across voices -- are then sorted by their
*score order*. This results in an iteration which moves first forward in time
and the top-of-score to bottom.

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = consort.MusicSpecifier(
    attachment_handler=consort.AttachmentHandler(),
    rhythm_maker=rhythmmakertools.TaleaRhythmMaker(
        extra_counts_per_division=(0, 1),
        talea=rhythmmakertools.Talea([2, 3, 2, 4], 16),
        ),
    )
timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea([0, 1], 4),
    playing_groupings=(1, 2, 2, 1, 2),
    playing_talea=rhythmmakertools.Talea([2, 3], 8),
    silence_talea=rhythmmakertools.Talea([1, 2, 3, 4], 8),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = consort.SegmentMaker(
    desired_duration_in_seconds=8,
    discard_final_silence=True,
    permitted_time_signatures=[(2, 4), (5, 16), (3, 4)],
    score_template=templatetools.GroupedRhythmicStavesScoreTemplate(
        staff_count=2,
        with_clefs=True,
        ),
    settings=[music_setting],
    tempo=indicatortools.Tempo((1, 4), 72),
    )
illustration = segment_maker(annotate=True, verbose=False)
show(illustration)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
for index, key_value_pair in enumerate(segment_maker.attack_point_map.items()):
    logical_tie, attack_point_signature = key_value_pair
    markup = Markup(index, Up)
    markup = markup.smaller().pad_around(0.5).box()
    attach(markup, logical_tie.head)

show(illustration)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
for index, key_value_pair in enumerate(segment_maker.attack_point_map.items()):
    logical_tie, attack_point_signature = key_value_pair
    for markup in inspect_(logical_tie.head).get_markup():
        detached = detach(markup, logical_tie.head)
    string = '{}:{}'.format(
        attack_point_signature.division_index,
        attack_point_signature.logical_tie_index,
        )
    markup = Markup(string, Up)
    markup = markup.smaller().pad_around(0.5).box()
    attach(markup, logical_tie.head)

show(illustration)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
for index, key_value_pair in enumerate(segment_maker.attack_point_map.items()):
    logical_tie, attack_point_signature = key_value_pair
    for markup in inspect_(logical_tie.head).get_markup():
        detached = detach(markup, logical_tie.head)
    phrase_position = attack_point_signature.phrase_position
    markup = Markup.fraction(phrase_position)
    markup = Markup(markup, Up)
    markup = markup.smaller().pad_around(0.5).box()
    attach(markup, logical_tie.head)

show(illustration)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
for index, key_value_pair in enumerate(segment_maker.attack_point_map.items()):
    logical_tie, attack_point_signature = key_value_pair
    for markup in inspect_(logical_tie.head).get_markup():
        detached = detach(markup, logical_tie.head)
    segment_position = attack_point_signature.segment_position
    markup = Markup.fraction(segment_position)
    markup = Markup(markup, Up)
    markup = markup.smaller().pad_around(0.5).box()
    attach(markup, logical_tie.head)

show(illustration)
</abjad>
\end{comment}

\subsection{Collecting attack points}

Many of the handlers used during non-rhythmic interpretation rely on
information about each pitched logical tie in the score.

Including the position of the logical tie's head in the score regardless of
voice -- its first leaf, and effectively its attack point -- as well as that
head's index both within the division it occurs within, as well as which
division within its phrase it occurs within.

\begin{markdown}
-   AttackPointSignature
    -   division_index
    -   phrase_index
    -   logical_tie_index
    -   phrase_position (scaled position within the phrase)
    -   division_position (scaled position within the division)
    -   segment_position (scaled position within the entire segment)
    -   is_first_of_division
    -   is_first_of_phrase
-   Elucidate voice / phrase / division structure
-   SegmentMaker.logical_tie_to_division()
-   SegmentMaker.logical_tie_to_phrase()
-   SegmentMaker.logical_tie_to_voice()
\end{markdown}

\subsection{Grace handlers}

Grace handlers attach *grace containers* to logical ties within a score in a
patterned way.

Abjad implements grace notes as normal leaves -- notes, chords and rests --
within special components which act both as Abjad \texttt{Container} classes as
well as attachable indicators. That is to say, grace notes must be placed
within one of these grace containers which is then attached to another leaf in
the score much like any other indicator, such as dynamics or articulations.

Grace-handlers traverse the score by logical tie in time-order, by iterating
over the previously cached ordered dictionary of attack-points, generated at
the end of rhythmic interpretation.

Discuss sub-optimal grace-note spacing in proportional notation?

\subsection{Pitch-handlers}
\label{ssec:pitch-handlers}

Pitch-handlers manage the process of applying pitch material to logical ties
within a score in a patterned way.

Pitch material may be semantic or non-semantic.

\begin{markdown}
-   Applies pitches to logical ties in a patterned way.
-   Applies pitches to grace notes associated with a logical tie.
-   Applies logical-tie-expressions which can convert logical ties from
    notes into chords, key-clusters or harmonics.
-   Processes the score time-wise by logical tie.
    -   The goal is to limit pitch class repetition both vertically and
        horizontally, but *only* with regard to phrases in the score scoped
        by each music specifier. Other phrases are not considered.
-   Application rate: by logical tie, division, phrase
    -   This requires the SeedSession class for keeping track of many
        seeds, each advancing at a potentially different rate.
    -   This also requires the AttackPointSignature class, which caches
        information about each logical tie's position in its parent
        division, phrase and overall segment.
-   MusicSpecifier: pitches are non-semantic (is this even used?)
-   Maps different patterns of pitches and different patterns of operations
    across the timeline.
    -   PitchHandler: `get_pitch_choice_timespans()`
    -   Demonstrate.
-   Can act on absolute pitches, or registered pitch classes.
-   Other formulations are possible: selecting from vertical sonorities
    based on register curves. (This is not currently implemented, but maybe
    for Ersilia.)
-   Demonstrate simple PitchHandler examples.
-   Additionally:
    -   SeedSession
    -   Pitch operations
    -   Logical tie expressions
    -   Pitch application rate
    -   Pitch specifier
    -   Grace expressions
\end{markdown}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
segment_maker = consort.SegmentMaker(
    desired_duration_in_seconds=9,
    #omit_stylesheets=True,
    permitted_time_signatures=[(3, 4)],
    score_template=templatetools.GroupedStavesScoreTemplate(
        staff_count=2,
        ),
    tempo=indicatortools.Tempo((1, 4), 60),
    )
music_specifier = consort.MusicSpecifier(
    rhythm_maker=rhythmmakertools.TaleaRhythmMaker(
        talea=rhythmmakertools.Talea([1], 16),
        ),
    )
timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea([0, 1], 4),
    playing_talea=rhythmmakertools.Talea([1], 8),
    playing_groupings=[3],
    silence_talea=rhythmmakertools.Talea([1], 8),
    )
segment_maker.add_setting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler=consort.AbsolutePitchHandler(
        pitch_specifier="c' d' e' f' g' a' b' c''",
        ),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    rhythm_maker__talea__counts=[1, 2, 3, 4],
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
other_music_specifier = consort.MusicSpecifier(
    pitch_handler=consort.AbsolutePitchHandler(pitch_specifier='g fs e f'),
    rhythm_maker=rhythmmakertools.EvenDivisionRhythmMaker(
        denominators=[8],
        extra_counts_per_division=(1,),
        ),
    )
other_music_setting = consort.MusicSetting(
    timespan_maker=consort.TaleaTimespanMaker(
        initial_silence_talea=rhythmmakertools.Talea([1], 2),
        silence_talea=rhythmmakertools.Talea([1], 2),
        ),
    v1=other_music_specifier,
    v2=other_music_specifier,
    )
segment_maker = new(
    segment_maker,
    settings=[music_setting, other_music_setting],
    )
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__pitch_specifier=consort.PitchSpecifier(
        pitch_segments=(
            "c' e' g'",
            "fs' g' a'",
            "b d'",
            ),
        ratio=(1, 2, 3),
        ),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__pitch_operation_specifier=consort.PitchOperationSpecifier(
        pitch_operations=(
            pitchtools.PitchOperation((
                pitchtools.Inversion(),
                )),
            None,
            pitchtools.PitchOperation((
                pitchtools.Rotation(-1),
                pitchtools.Transposition(-1),
                ))
            ),
        ratio=(1, 2, 1),
        ),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__forbid_repetitions=True,
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__logical_tie_expressions=(
        consort.ChordExpression(chord_expr=(-2, 0, 2)),
        consort.ChordExpression(chord_expr=(-7, 0, 7)),
        None,
        ),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
segment_maker = new(
    segment_maker,
    settings=[music_setting, other_music_setting],
    )
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = consort.MusicSpecifier(
    pitch_handler=consort.AbsolutePitchHandler(
        pitch_application_rate='division',
        pitch_specifier="c' d' e' f' g' a' b' c''",
        ),
    rhythm_maker=rhythmmakertools.EvenDivisionRhythmMaker(denominators=[16]),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
lilypond_file = segment_maker(verbose=False)
consort.annotate(lilypond_file.score)
show(lilypond_file)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__pitch_application_rate='phrase',
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
lilypond_file = segment_maker(verbose=False)
consort.annotate(lilypond_file.score)
show(lilypond_file)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__deviations=(0, 0, '-m2', '+m2'),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
lilypond_file = segment_maker(verbose=False)
consort.annotate(lilypond_file.score)
show(lilypond_file)
</abjad>
\end{comment}

\subsection{Attachment-handlers}
\label{ssec:attachment-handlers}

Attachment-handlers manage the process of attaching indicators and spanners to
selections of components within a score. They may also evaluate *component
expressions* against selections of components, much like the logical tie
expressions outlined in \autoref{ssec:pitch-handlers}.

Attachment-handlers aggregate *attachment expressions* by associating those
expressions with underscore-delimited string keys. This mechanism, nearly
identical to that employed by music settings, allows attachment expressions --
which may have an arbitrary number of such associations -- to be reconfigured
through templating to add new attachment expressions or overwrite or nullify
specific existing expressions.\footnote{The principle employed by both music
settings and attachment handlers is crucial: named references beat positional
references.}

Attachment expressions pair a *component selector* -- as described in
\autoref{ssec:selectors} -- and an iterable of attachments -- indicators and
spanners -- or component expressions.

Attachment-handlers provide various niceties for instantiation.

\begin{markdown}
-   Attaches things to the score.
-   Aggregates AttachmentExpression instances together.
    -   A bundle of a selector and an iterable of attachments or
        expressions.
    -   Reprise discussion of selectors.
-   Processes the score by voice, then by *phrase*.
    -   Unlike the other handlers, attachment handlers require the entire
        phrase to operate on, and selectors should be designed with that in
        mind.
\end{markdown}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = consort.MusicSpecifier(
    attachment_handler=consort.AttachmentHandler(),
    rhythm_maker=rhythmmakertools.TaleaRhythmMaker(
        extra_counts_per_division=(0, 1),
        talea=rhythmmakertools.Talea([1, 2, 3, 1, 4], 16),
        ),
    )
timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea([0, 1], 4),
    playing_groupings=(1, 2, 2),
    playing_talea=rhythmmakertools.Talea([2, 3], 8),
    silence_talea=rhythmmakertools.Talea([1, 2, 3, 4], 8),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = consort.SegmentMaker(
    desired_duration_in_seconds=8,
    discard_final_silence=True,
    permitted_time_signatures=[(2, 4), (5, 16), (3, 4)],
    score_template=templatetools.GroupedRhythmicStavesScoreTemplate(
        staff_count=2,
        with_clefs=True,
        ),
    settings=[music_setting],
    tempo=indicatortools.Tempo((1, 4), 72),
    )
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    attachment_handler__accents=consort.AttachmentExpression(
        attachments=Articulation('accent'),
        selector=selectortools.Selector().by_leaves()[0],
        ),
    )
music_setting = new(
    music_setting,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker,verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    attachment_handler__tenuti=consort.AttachmentExpression(
        attachments=Articulation('tenuto'),
        selector=selectortools.Selector()
            .by_leaves()[1:]
            .by_logical_tie(pitched=True)[0],
        ),
    )
music_setting = new(
    music_setting,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    attachment_handler__slurs=Slur()
    )
music_setting = new(
    music_setting,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    attachment_handler__dynamics=consort.DynamicExpression(['f', 'p'])
    )
music_setting = new(
    music_setting,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
lilypond_file = segment_maker(verbose=False)
consort.annotate(lilypond_file.score)
show(lilypond_file)
</abjad>
\end{comment}

\subsection{Expressive attachments}
\label{ssec:expressive-attachments}

\begin{markdown}
-   Idiomatic indicators
-   DynamicExpression
-   BowContactSpanner
-   StringContactSpanner
-   Indicators attached as annotations
-   Spanners as a kind of clever post-processing
\end{markdown}

\subsection{Post-processing}
\label{ssec:post-processing}

Score configuration.

Adding a time signature context, as described in
\autoref{ssec:score-post-processing}.

Consort also allows for inserting a LilyPond \texttt{\\include} statement at
the top of any LilyPond file constructed through interpretation which points to
a stylesheet located relative to a path defined on the segment-maker.

By subclasses Consort's segment-maker, individual score packages can cause
their own segment-makers to automatically locate the appropriate stylesheet
file relative to the package where that segment-maker subclass was defined.

A time-signature context is created and inserted at the top of the score if the
score does not already contain such a context. The segment-maker populates this
time-signature context with measures filled with typographic spacer-skips, to
create the appearance of floating time-signature indications.

\begin{markdown}
-   Score configuration
    -   Time signature context
        -   Adding time signatures
        -   Rehearsal marks
        -   Repeat signs
-   LilyPond configuration
    -   Style-sheets
\end{markdown}

\subsection{Voice copying}
\label{ssec:voice-copying}

For reasons related solely to how LilyPond handles various typographic
constructs during typesetting, it may prove necessary to copy certain voices in
the score, maintaining all rhythmic information therein, but changing their
context name and filtering out various spanners and indicators so that only a
specific subset of typographic commands remain.

My score \emph{Invisible Cities (ii): Armilla} employs this voice-copying
technique to create the desired typography.

This is achieved by subclassing Consort's segment-maker.

Voice-copying treats the output of interpretation as \enquote{semantic}, but
not necessarily fully-ready for typesetting.

\section{Persistence \& visualization}

Once interpreted, a segment-maker's illustration may be persisted to disk as
LilyPond syntax for inclusion in other LilyPond files, rendered as a PDF for
viewing, or even serialized for other purposes. Composers study the results of
interpretation, make changes to each segment's specification, and re-interpret
as necessary, a large-scale re-enactment of interactive programming's pervasive
\emph{read-eval-print} loop paradigm.