%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{\emph{Consort}: a model of composition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
<abjad>[hide=true]
import collections
import consort
</abjad>
\end{comment}

\begin{markdown}

#   Specification & Interpretation

-   Materials
-   Configuration
-   Templating
-   Layers
-   Coarse to fine
-   Rhythm first

What is specification? What is a specifier? What is configuration and
aggregation?

What should happen musically, where should it happen?

What is material?

What is music?

Rhythm is interpreted first, as all other parameters depend on it.

Rhythm is interpreted from "coarse" to "fine": from the level of phrase
boundaries to the level of individual notes, rests, tuplets and ties.

This discussion only focuses on notation, nothing related to aesthetic
experience, physical modeling or anything else. This is a tool for a specific
composer to create scores, not a discussion explicitly of why they would work
this way (although that should be discussed in the conclusion).

Specification and interpretation conceive of the score as a single, hugely
complex expression.

Templating as variation.

#   Overview: specification

-   Segments

#   Segment-makers

-   Permitted time signatures
-   Tempo
-   Desired duration
-   Score template
    -   LilyPond will automatically concatenate scores with identical
        context hierarchies. All contexts need to be present in every
        concatenated segment, otherwise LilyPond will concatentate
        incorrectly. However, we can use various typographic overrides to
        make it appear that a context has disappeared.
    -   Consort's ScoreTemplateManager helps create regularized score
        templates.
-   Music settings

#   Music settings

-   What kind of music, laid out in what pattern, in which general
    timeframe, for which voices, in what layer?
-   A timespanMaker
-   A target timespan
    -   A timespan.
    -   A timespan inventory.
    -   A ratio/parts expression.
        -   Demonstrate. Check.
-   Voice abbreviation / music specifier pairs
    -   Why abbreviate? It's because of Python's key=value syntax for
        keyword arguments.
    -   How do we convert abbreviation to an actual context name?
        -   Score templates need to define a mapping.
        -   Demonstrate. Check.
    -   What is the context name for?
        -   Name-wise indexing in an actual score.
        -   Demonstrate. Check.
    -   Also, composite music specifiers: when two contexts need to be
        discussed as a single unit.
        -   Composite music specifiers need to convert one abbreviation into
            two more abbreviations, each mapped to an actual context name.
        -   Consort's ScoreTemplateManager takes care of calculating and
            caching the appropriate names and abbreviations.
-   Layer is implicit, derived from the order of setting definitions.
    -   Recall the discussion of timespan layer from earlier chapters.

# Music specifiers

-   A bundle of descriptors for what kind of music should fill a series of
    1 or more timespans.
-   All of the descriptors are optional.
-   Rhythm-makers:
    -   Creates rhythms in the divisions defined by a series of contiguous
        timespans.
-   Grace-handlers:
    -   Adds grace notes to the start of logical ties in a patterned way.
    -   Processes the score timewise by logical tie.
-   Pitch-handlers:
    -   Applies pitches to logical ties in a patterned way.
    -   Applies pitches to grace notes associated with a logical tie.
    -   Applies logical-tie-expressions which can convert logical ties from
        notes into chords, key-clusters or harmonics.
    -   Processes the score timewise by logical tie.
        -   The goal is to limit pitch class repetition both vertically and
            horizontally, but *only* with regard to phrases in the score scoped
            by each music specifier. Other phrases are not considered.
    -   Application rate: by logical tie, division, phrase
        -   This requires the SeedSession class for keeping track of many
            seeds, each advancing at a potentially different rate.
        -   This also requires the AttackPointSignature class, which caches
            information about each logical tie's position in its parent
            division, phrase and overall segment.
    -   MusicSpecifier: pitches are nonsemantic (is this even used?)
    -   Maps different patterns of pitches and different patterns of operations
        across the timeline.
        -   PitchHandler: `get_pitch_choice_timespans()`
        -   Demonstrate.
    -   Can act on absolute pitches, or registered pitch classes.
    -   Other formulations are possible: selecting from vertical sonorities
        based on register curves. (This is not currently implemented, but maybe
        for Ersilia.)
-   Attachment-handlers:
    -   Attaches things to the score.
    -   Aggregates AttachmentExpressions together.
        -   A bundle of a selector and an iterable of attachments or
            expressions.
        -   Reprise discussion of selectors.
        -   Discuss expressions: DynamicExpression, etc.
    -   Processes the score by voice, then by *phrase*.
        -   Unlike the other handlers, attachment handlers require the entire
            phrase to operate on, and selectors should be designed with that in
            mind.
    -   Demonstrate a gallery of selectors.
        -   by duration
        -   by leaves
        -   by length
        -   by logical tie
        -   by counts (with negative counts too)
    -   Expressive attachments
        -   Idiomatic indicators
        -   DynamicExpression
        -   BowContactSpanner
        -   StringContactSpanner
-   Other music specifier properties of note?
    -   *labels*
        -   This works in tandem with DependentTimespanMaker.
        -   Example? Piano music with two hands and pedaling. The
            hand-performed music might involve key presses, or it might involve
            percussive techniques. All of the techniques that require pedaling
            should be labeled 'pedaled'. A DependentTimespanMaker for the pedal
            context can then be configured to look at timespans for both the LH
            and RH of the piano, but only those timespans configured with a
            music specifier labeled 'pedaled'.
        -   Demonstrate.
    -   *minimum phrase duration*
        -   (should this be hoisted into PerformedTimespan?
    -   *seed*
    -   *is_sentinel* (maybe remove from consort altogether)

# Composite music specifiers

# Overview: interpretation

-   Populating independent timespans
-   Populating dependent timespans
-   Populating silent timespans
-   Validating timespans
-   Rewriting meter
-   Populating the score
-   Collecting attack points

# Timespan population

-   Multiplexing & demultiplexing
-   Resolving cascading overlap

# Timespan inscription

-   Divisions
-   Rhythm creation
-   Rest consolidation
-   Rhythmic post-processing
    -   attaching a GeneralizedBeam
    -   attaching the scoped music specifier
-   Meter rewriting
    Cleaning up logical ties
-   Attack-point collection

# Post-rhythm-creation processing

-   Score traversal revisited
-   Grace handling
-   Pitch handling
    -   SeedSession
    -   Pitch operations
    -   Logical tie expressions
    -   Pitch application rate
    -   Pitch specifier
    -   Grace expressions
-   Attachment handling

# Score post-processing

-   Voice copying

\end{markdown}

\section{Examples}

\subsection{Ratio-parts expression}

\begin{comment}
<abjad>
timespan = timespantools.Timespan(0, 8)
show(timespan)
ratio_parts_expression = consort.RatioPartsExpression(
    ratio=[1, 2, 1],
    parts=[0, 2],
    )
result = ratio_parts_expression(timespan)
show(result)
</abjad>
\end{comment}

\begin{comment}
<abjad>
ratio_parts_expression = new(
    ratio_parts_expression,
    timespan=timespantools.Timespan(
        start_offset=Offset(1, 4),
        ),
    )
result = ratio_parts_expression(timespan)
show(result, range_=(0, 8))
</abjad>
\end{comment}

\subsection{Score templates and abbreviations}

\begin{comment}
<abjad>
import armilla
armilla_score_template = armilla.makers.ArmillaScoreTemplate()
armilla_score = armilla_score_template()
print(format(armilla_score))
for item in armilla_score_template.context_name_abbreviations.items():
    abbreviation, context_name = item
    print(abbreviation, context_name)

for item in armilla_score_template.composite_context_pairs.items():
    grouping_abbreviation, grouped_abbreviations = item
    print(grouping_abbreviation, grouped_abbreviations)

</abjad>
\end{comment}

\begin{comment}
<abjad>
armilla_score['Viola 1 Fingering Voice']
</abjad>
\end{comment}

\subsection{Pitch choice timespans}

\begin{comment}
<abjad>

</abjad>
\end{comment}

\subsection{Music specifier labels}

\begin{comment}
<abjad>
unlabeled_music_specifier = consort.MusicSpecifier()
labeled_music_specifier = consort.MusicSpecifier(labels=['labeled'])
timespan_inventory = timespantools.TimespanInventory([
    consort.PerformedTimespan(
        layer=1,
        start_offset=0,
        stop_offset=4,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=2,
        stop_offset=7,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 2',
        ),
    consort.PerformedTimespan(
        layer=2,
        start_offset=6,
        stop_offset=8,
        music_specifier=unlabeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=2,
        start_offset=10,
        stop_offset=(25, 2),
        music_specifier=unlabeled_music_specifier,
        voice_name='Voice 2',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=11,
        stop_offset=14,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=14,
        stop_offset=16,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=15,
        stop_offset=16,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 2',
        ),
    ])
show(timespan_inventory, key='voice_name')
</abjad>
\end{comment}

\begin{comment}
<abjad>
dependent_timespan_maker = consort.DependentTimespanMaker(
    include_inner_starts=True,
    voice_names=('Voice 1', 'Voice 2'),
    )
result = dependent_timespan_maker(
    layer=3,
    music_specifiers={'Voice 3': None},
    timespan_inventory=timespan_inventory[:],
    )
show(result, key='voice_name')
</abjad>
\end{comment}

\begin{comment}
<abjad>
dependent_timespan_maker = new(
    dependent_timespan_maker,
    labels=['labeled'],
    )
result = dependent_timespan_maker(
    layer=3,
    music_specifiers={'Voice 3': None},
    timespan_inventory=timespan_inventory[:],
    )
show(result, key='voice_name')
</abjad>
\end{comment}

\subsection{Timespan resolution}

\begin{comment}
<abjad>
layer_1_timespan_maker = consort.FloodedTimespanMaker()
layer_1_target_timespan = timespantools.Timespan(0, (19, 4))
layer_1_music_specifiers = collections.OrderedDict([
    ('Voice 2', None),
    ('Voice 3', None),
    ])
layer_1 = layer_1_timespan_maker(
    layer=1,
    music_specifiers=layer_1_music_specifiers,
    target_timespan=layer_1_target_timespan,
    )
show(layer_1, key='voice_name', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{comment}
<abjad>
layer_2_timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea(
        counts=(0, 1, 3),
        denominator=8,
        ),
    playing_groupings=(1, 2),
    playing_talea=rhythmmakertools.Talea(
        counts=(1, 2, 3, 4),
        denominator=4,
        ),
    silence_talea=rhythmmakertools.Talea(
        counts=(5, 3, 1),
        denominator=8,
        ),
    )
layer_2_target_timespan = timespantools.Timespan((3, 4), (19, 4))
layer_2_music_specifiers = collections.OrderedDict([
    ('Voice 1', None),
    ('Voice 2', None),
    ('Voice 3', None),
    ('Voice 4', None),
    ])
layer_2 = layer_2_timespan_maker(
    layer=2,
    music_specifiers=layer_2_music_specifiers,
    target_timespan=layer_2_target_timespan,
    )
show(layer_2, key='voice_name', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{comment}
<abjad>
layer_3_timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea(
        counts=(0, 0, 0, 1),
        denominator=8,
        ),
    padding=Duration(1, 4),
    playing_talea=rhythmmakertools.Talea(
        counts=(2, 3, 4),
        denominator=8,
        ),
    silence_talea=rhythmmakertools.Talea(
        counts=(6,),
        denominator=4,
        ),
    synchronize_step=True,
    )
layer_3_target_timespan = timespantools.Timespan((6, 4), (21, 4))
layer_3_music_specifiers = collections.OrderedDict([
    ('Voice 1', None),
    ('Voice 3', None),
    ('Voice 4', None),
    ])
layer_3 = layer_3_timespan_maker(
    layer=3,
    music_specifiers=layer_3_music_specifiers,
    target_timespan=layer_3_target_timespan,
    )
show(layer_3, key='voice_name', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{comment}
<abjad>
timespan_makers = (
    layer_1_timespan_maker,
    layer_2_timespan_maker,
    layer_3_timespan_maker,
    )
music_specifiers = (
    layer_1_music_specifiers,
    layer_2_music_specifiers,
    layer_3_music_specifiers,
    )
target_timespans = (
    layer_1_target_timespan,
    layer_2_target_timespan,
    layer_3_target_timespan,
    )
triples = zip(timespan_makers, music_specifiers, target_timespans)
timespan_inventory = timespantools.TimespanInventory()
for layer, triple in enumerate(triples, 1):
    timespan_inventory = triple[0](
        layer=layer,
        music_specifiers=triple[1],
        target_timespan=triple[2],
        timespan_inventory=timespan_inventory,
        )

show(timespan_inventory, key='voice_name', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{comment}
<abjad>
show(timespan_inventory, range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{comment}
<abjad>
demultiplexed_timespans = consort.TimeManager.demultiplex_timespans(
    timespan_inventory,
    )
show(demultiplexed_timespans, range_=(0, (21, 4)))
</abjad>
\end{comment}

\subsection{Rest consolidation}

\begin{comment}
<abjad>
music = scoretools.Container(r'''
    { r4 c'8 }
    \times 2/3 { d'4 r8 }
    { r4 e'4 f'4 r4 }
    { r4 g8 r8 }
    { r4 }
    { r4 }
    { a'4 \times 2/3 { b'4 r8 } }
    { c''4 r8 }
    ''')
print(format(music))
show(music)
</abjad>
\end{comment}

\begin{comment}
<abjad>
music = consort.TimeManager.consolidate_rests(music)
print(format(music))
show(music)
</abjad>
\end{comment}

\begin{comment}
<abjad>
parseable = r'''
\new Voice {
    {
        \time 4/4
        r4
        c4
    }
    \times 2/3 {
        c4
        r8
    }
    {
        \time 5/4
        r4
        c2 ~
        c8
        r8
    }
    {
        r4
    }
    {
        \time 3/4
        r4
    }
    {
        c8
        c4.
        \times 2/3 {
            \time 2/4
            c4
            r4
            c4
        }
        \time 4/4
        c4.
        r8
    }
    {
        r8
        c4
        r8
    }
}
'''
staff = Staff(parseable, context_name='RhythmicStaff')
show(staff)
</abjad>
\end{comment}
