%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{\emph{Consort}: a model of composition}
\label{chap:a-model-of-composition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{\textbf{TODO:} Elaborate on motivation for Consort.}

\begin{comment}
<abjad>[hide=true]
import collections
import consort
</abjad>
\end{comment}

\begin{comment}
\begin{markdown}
-   Materials
-   Configuration
-   Templating
-   Layers
-   Coarse to fine
-   Rhythm first
-   What is specification? What is a specifier? What is configuration and
    aggregation?
-   What should happen musically, where should it happen?
-   What is material?
-   What is music?
-   Rhythm is interpreted first, as all other parameters depend on it.
-   Rhythm is interpreted from "coarse" to "fine": from the level of phrase
    boundaries to the level of individual notes, rests, tuplets and ties.
-   This discussion only focuses on notation, nothing related to aesthetic
    experience, physical modeling or anything else. This is a tool for a
    specific composer to create scores, not a discussion explicitly of why they
    would work this way (although that should be discussed in the conclusion).
-   Specification and interpretation conceive of the score as a single, hugely
    complex expression.
-   Templating as variation.
-   Define what composition means here: laying out symbols on the page.
-   This way of thinking and working does not attempt to define or even model
    concepts like "melody" or even "phrase". They're too vague. If we use that
    terminology at all, it is only in the most incredibly constrained way.
\end{markdown}
\end{comment}

\eblettrinedbl{C}{onsort, a Python library} I have written as an extension to
Abjad, models the process of composing of notated musical scores in terms of a
repeated cycle containing two distinct stages: \emph{specification},
\emph{interpretation}. What follows is a detailed analysis of the various
algorithms and subroutines employed during Consort's specification and
interpretation stages.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Specification}
\label{sec:specification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\emph{Specification} describes how \emph{out-of-time materials} -- both
concrete and programmatic -- should be deployed \emph{in-time} in a
\emph{segment} of musical score as notation. Materials encompass abstractions
-- such as pitch sets or collections of performance technique indications --,
concrete fragments of \emph{music} -- narrowly defined here as any contiguous
selection of score components --, and procedures for producing, altering or
embellishing music such as rhythm-makers or attachment-handlers. Score segments
comprise any contiguous passage of music, demarcating an area of compositional
concern. Consort treats scores as comprised of at least one segment, but
potentially many more concatenated together. Any segment may of course contain
arbitrarily complex inner structuring. Separation of scores into distinct
segments acts then mainly as an aid for the composer, both by simplifying the
complexity of the current specification under consideration, and by allowing
the typesetting engine -- LilyPond -- to display more manageable amounts of
notation than the full score, thus speeding up the cycle of specifying,
interpreting and visualizing.

\subsection{Segment-makers}
\label{ssec:segment-makers}

Composers specify segments by creating and progressively configuring
\emph{segment-makers}, classes which conceptually mirror the rhythm- and
timespan-makers described in \autoref{chap:time-tools}, but on a much larger
scale. Such configuration parameters include tempo, permitted meters, desired
duration, and score template. Score templates are notation factories which
build scores comprised of staff groups, staves, voices, clefs and instruments,
as necessary to model the context hierarchy of a score to which no count-time
components have yet been added. All segment-makers in a score project must use
the same score template if they are to appear contiguously in the complete
typeset score. LilyPond will automatically concatenate scores with identical
context hierarchies. All contexts need to be present in every concatenated
segment, otherwise LilyPond will concatenate incorrectly. However, various
typographic overrides can be employed to make it seem that a context has
disappeared.

The following defines a segment-maker with desired duration of 9 seconds, only
\sfrac[big]{3}{4} meters permitted, a score-template comprised of two rhythmic
staves, and a tempo or quarter-equals-60:

\begin{comment}
<abjad>
segment_maker = consort.SegmentMaker(
    desired_duration_in_seconds=9,
    permitted_time_signatures=[(3, 4)],
    score_template=templatetools.GroupedRhythmicStavesScoreTemplate(
        staff_count=2,
        with_clefs=True,
        ),
    tempo=indicatortools.Tempo((1, 4), 60),
    )
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> segment_maker = consort.SegmentMaker(
...     desired_duration_in_seconds=9,
...     permitted_time_signatures=[(3, 4)],
...     score_template=templatetools.GroupedRhythmicStavesScoreTemplate(
...         staff_count=2,
...         with_clefs=True,
...         ),
...     tempo=indicatortools.Tempo((1, 4), 60),
...     )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent This segment-maker can be illustrated via \texttt{show()}, like many
other objects in Abjad. Illustration here invokes the segment-maker's
interpretation stage. The \texttt{verbose=False} flag prevents it from
printing a considerable amount of diagnostic information:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-ef0ad575af796cf07129840810211361.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent By changing the tempo from quarter-equals-60 to quarter-equals-20,
the overall notated duration of the segment shrinks by two thirds, but the
duration in seconds remains the same. This mechanism allows segments to be
planned relative one another in terms of their \enquote{actual} durations:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
slower_segment_maker = new(
    segment_maker,
    tempo=indicatortools.Tempo((1, 4), 20),
    )
show(slower_segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> slower_segment_maker = new(
...     segment_maker,
...     tempo=indicatortools.Tempo((1, 4), 20),
...     )
>>> show(slower_segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-e442eacf61204e961271e05f093b7b09.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Most importantly, segment-makers may be configured with any number of
\emph{music settings}, which object-model both \emph{when} and in \emph{which}
voices musical materials should be deployed.

\subsection{Music settings}
\label{ssec:music-settings}

Music settings represent a layer of musical texture, in one or more voices, of
arbitrary length. Music settings aggregate a timespan-maker, a target timespan
and any number of \emph{music specifiers}. The timespan-maker provides the
overall phrasing and density structure, the optional timespan identifier
defines in what portion of the current segment the timespan-maker's texture
should be spooled out, and the music specifiers define both in which voices the
timespan texture should appear as well as how those timespans should ultimately
be rendered as notation. The order in which composers configure segment-makers
with music settings defines each music setting's \emph{layer} -- the first
setting defined being layer 0, the second layer 1 and so forth, with each
higher layer number indicating higher precedence or \enquote{foregroundness}
--, determining how overlapping events in a single voice will mask one another.
The timespans created by music settings which are defined later during segment
specification \enquote{hide} any timespans created by those music settings
defined earlier. Score materials, including music settings, music specifiers,
timespan-makers and any other class pertinent to score creation -- potentially
even other segment-makers, may be defined from scratch in the same code module
as the segment-maker currently being configured, templated from another
material, or simply imported into the segment definition's namespace.

\subsection{Music specifiers}
\label{ssec:music-specifiers}

Music specifiers bundle all of the information necessary for a segment-maker to
generate the notational content for a sequence of one or more divisions,
grouped as a phrase. This information includes optional rhythm-maker
(\autoref{sec:rhythm-makers}), \emph{grace-handler}
(\autoref{ssec:grace-handlers}), \emph{pitch-handler}
(\autoref{ssec:pitch-handlers}) and \emph{attachment-handler}
(\autoref{ssec:attachment-handlers}) definitions -- all classes which describe
strategies for creating or modifying notation --, as well as a variety of other
properties including an optional \emph{minimum phrase duration} -- described
further in \autoref{ssec:splitting-pruning-and-consolidation} --, and
\emph{seed}.

Music specifiers can also be configured with a \emph{labels}: a tuple of one or
more arbitrary strings, identifying some quality of those performed timespans.
As demonstrated in \autoref{ssec:dependent-timespan-makers}, composers
configure dependent timespan-makers in order to create timespans according to
the disposition of performed timespans associated with specific voices.
Dependent timespan-makers can also be configured to select depended-upon
timespans based on the \texttt{labels} property of those performed timespans'
music specifiers, allowing an additional category for filtering. This helps
model creating a pedal voice mirroring not simply a pianist's
left- and right-hand events, but only those that actually depress keys,
ignoring guero events or other off-the-key percussive techniques for which no
pedaling is desired.

Consider the following timespan inventory, populated with performed timespans
associated with one of two voices, and annotated with music specifiers which
are either labeled or not:

\begin{comment}
<abjad>
unlabeled_music_specifier = consort.MusicSpecifier()
labeled_music_specifier = consort.MusicSpecifier(labels=['labeled'])
timespan_inventory = timespantools.TimespanInventory([
    consort.PerformedTimespan(
        layer=1,
        start_offset=0,
        stop_offset=4,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=2,
        stop_offset=7,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 2',
        ),
    consort.PerformedTimespan(
        layer=2,
        start_offset=6,
        stop_offset=8,
        music_specifier=unlabeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=2,
        start_offset=10,
        stop_offset=(25, 2),
        music_specifier=unlabeled_music_specifier,
        voice_name='Voice 2',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=11,
        stop_offset=14,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=14,
        stop_offset=16,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 1',
        ),
    consort.PerformedTimespan(
        layer=1,
        start_offset=15,
        stop_offset=16,
        music_specifier=labeled_music_specifier,
        voice_name='Voice 2',
        ),
    ])
show(timespan_inventory, key='voice_name')
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> unlabeled_music_specifier = consort.MusicSpecifier()
>>> labeled_music_specifier = consort.MusicSpecifier(labels=['labeled'])
>>> timespan_inventory = timespantools.TimespanInventory([
...     consort.PerformedTimespan(
...         layer=1,
...         start_offset=0,
...         stop_offset=4,
...         music_specifier=labeled_music_specifier,
...         voice_name='Voice 1',
...         ),
...     consort.PerformedTimespan(
...         layer=1,
...         start_offset=2,
...         stop_offset=7,
...         music_specifier=labeled_music_specifier,
...         voice_name='Voice 2',
...         ),
...     consort.PerformedTimespan(
...         layer=2,
...         start_offset=6,
...         stop_offset=8,
...         music_specifier=unlabeled_music_specifier,
...         voice_name='Voice 1',
...         ),
...     consort.PerformedTimespan(
...         layer=2,
...         start_offset=10,
...         stop_offset=(25, 2),
...         music_specifier=unlabeled_music_specifier,
...         voice_name='Voice 2',
...         ),
...     consort.PerformedTimespan(
...         layer=1,
...         start_offset=11,
...         stop_offset=14,
...         music_specifier=labeled_music_specifier,
...         voice_name='Voice 1',
...         ),
...     consort.PerformedTimespan(
...         layer=1,
...         start_offset=14,
...         stop_offset=16,
...         music_specifier=labeled_music_specifier,
...         voice_name='Voice 1',
...         ),
...     consort.PerformedTimespan(
...         layer=1,
...         start_offset=15,
...         stop_offset=16,
...         music_specifier=labeled_music_specifier,
...         voice_name='Voice 2',
...         ),
...     ])
>>> show(timespan_inventory, key='voice_name')
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-455efe8da98866e16be71eac66f13851.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent A dependent timespan-maker, configured to select timespans associated
with \enquote{Voice 1} and \enquote{Voice 2} produces dependent timespans in
the manner illustrated in \autoref{ssec:dependent-timespan-makers}:

\begin{comment}
<abjad>
dependent_timespan_maker = consort.DependentTimespanMaker(
    include_inner_starts=True,
    voice_names=('Voice 1', 'Voice 2'),
    )
result = dependent_timespan_maker(
    layer=3,
    music_specifiers={'Voice 3': None},
    timespan_inventory=timespan_inventory[:],
    )
show(result, key='voice_name')
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> dependent_timespan_maker = consort.DependentTimespanMaker(
...     include_inner_starts=True,
...     voice_names=('Voice 1', 'Voice 2'),
...     )
>>> result = dependent_timespan_maker(
...     layer=3,
...     music_specifiers={'Voice 3': None},
...     timespan_inventory=timespan_inventory[:],
...     )
>>> show(result, key='voice_name')
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-9fccfe458cfc733d4334be91fa574c0a.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Reconfiguring the above dependent timespan-maker to additionally
filter timespans whose music specifier is labeled \enquote{labeled} produces a
more restricted output. Note that the unlabeled \sfrac[big]{6}{1}-\sfrac[big]{8}{1}
timespan in \enquote{Voice 1} and the \sfrac[big]{10}{1}-\sfrac[big]{25}{2} timespan in
\enquote{Voice 2} are ignored:

\begin{comment}
<abjad>
dependent_timespan_maker = new(
    dependent_timespan_maker,
    labels=['labeled'],
    )
result = dependent_timespan_maker(
    layer=3,
    music_specifiers={'Voice 3': None},
    timespan_inventory=timespan_inventory[:],
    )
show(result, key='voice_name')
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> dependent_timespan_maker = new(
...     dependent_timespan_maker,
...     labels=['labeled'],
...     )
>>> result = dependent_timespan_maker(
...     layer=3,
...     music_specifiers={'Voice 3': None},
...     timespan_inventory=timespan_inventory[:],
...     )
>>> show(result, key='voice_name')
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-eb9cc16ea82d072878db44dff8aea3ae.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Music specifiers can be grouped into sequences called,
unsurprisingly, music specifier sequences, allowing a music setting to specify
that the timespans it creates associated with a certain voice should be
annotated with multiple different music specifiers in a patterned way:

\begin{comment}
<abjad>
music_specifier_sequence = consort.MusicSpecifierSequence(
    music_specifiers=['A', 'B', 'C'],
    )
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier_sequence = consort.MusicSpecifierSequence(
...     music_specifiers=['A', 'B', 'C'],
...     )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Recall from \autoref{ssec:talea-timespan-makers} that talea
timespan-makers can create contiguous groups of 1 or more timespan associated
with a specific voice. Music specifier sequences can be used to annotated a
different music specifier to each timespan in a contiguous group, or to
annotated a different music specifier to \emph{each} contiguous group. This
behavior is controlled via the music specifier's \emph{application rate}
property:

\begin{comment}
<abjad>
music_specifiers = {'Voice': music_specifier_sequence}
target_timespan = timespantools.Timespan(0, (7, 4))
timespan_maker = consort.TaleaTimespanMaker(
    playing_groupings=(3,),
    )
timespan_inventory = timespan_maker(
    music_specifiers=music_specifiers,
    target_timespan=target_timespan,
    )
print(format(timespan_inventory))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifiers = {'Voice': music_specifier_sequence}
>>> target_timespan = timespantools.Timespan(0, (7, 4))
>>> timespan_maker = consort.TaleaTimespanMaker(
...     playing_groupings=(3,),
...     )
>>> timespan_inventory = timespan_maker(
...     music_specifiers=music_specifiers,
...     target_timespan=target_timespan,
...     )
>>> print(format(timespan_inventory))
timespantools.TimespanInventory(
    [
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(0, 1),
            stop_offset=durationtools.Offset(1, 4),
            music_specifier='A',
            voice_name='Voice',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(1, 4),
            stop_offset=durationtools.Offset(1, 2),
            music_specifier='A',
            voice_name='Voice',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(1, 2),
            stop_offset=durationtools.Offset(3, 4),
            music_specifier='A',
            voice_name='Voice',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(1, 1),
            stop_offset=durationtools.Offset(5, 4),
            music_specifier='B',
            voice_name='Voice',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(5, 4),
            stop_offset=durationtools.Offset(3, 2),
            music_specifier='B',
            voice_name='Voice',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(3, 2),
            stop_offset=durationtools.Offset(7, 4),
            music_specifier='B',
            voice_name='Voice',
            ),
        ]
    )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Changing the application rate from the default value of
\enquote{phrase} to \enquote{division} causes a different music specifier from
the sequence to be annotated to each timespan, rather than each contiguous
group of timespans:

\begin{comment}
<abjad>
music_specifier_sequence = new(
    music_specifier_sequence,
    application_rate='division',
    )
music_specifiers = {'Voice': music_specifier_sequence}
timespan_inventory = timespan_maker(
    music_specifiers=music_specifiers,
    target_timespan=target_timespan,
    )
print(format(timespan_inventory))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier_sequence = new(
...     music_specifier_sequence,
...     application_rate='division',
...     )
>>> music_specifiers = {'Voice': music_specifier_sequence}
>>> timespan_inventory = timespan_maker(
...     music_specifiers=music_specifiers,
...     target_timespan=target_timespan,
...     )
>>> print(format(timespan_inventory))
timespantools.TimespanInventory(
    [
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(0, 1),
            stop_offset=durationtools.Offset(1, 4),
            music_specifier='A',
            voice_name='Voice',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(1, 4),
            stop_offset=durationtools.Offset(1, 2),
            music_specifier='B',
            voice_name='Voice',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(1, 2),
            stop_offset=durationtools.Offset(3, 4),
            music_specifier='C',
            voice_name='Voice',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(1, 1),
            stop_offset=durationtools.Offset(5, 4),
            music_specifier='B',
            voice_name='Voice',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(5, 4),
            stop_offset=durationtools.Offset(3, 2),
            music_specifier='C',
            voice_name='Voice',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(3, 2),
            stop_offset=durationtools.Offset(7, 4),
            music_specifier='A',
            voice_name='Voice',
            ),
        ]
    )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Consort provides an additional variation of music specifier called a
\emph{composite music specifier}.
Composite music specifiers allow for the definition of music for voices in
tandem, such as the fingering- and bowing-voice in a split-hands string
notation. When passed as part of a voice-name-to-music-specifier mapping to a
timespan-maker, that timespan-maker will create timespans for the first voice
of the composite music specifier and then create timespans for the second voice
as though it had its own dependent timespan-maker solely for those timespans.
This behavior ensures a degree of synchronization between pairs of voices which
should always appear at the same time in a score:

\begin{comment}
<abjad>
composite_music_specifier = consort.CompositeMusicSpecifier(
    primary_music_specifier='one',
    primary_voice_name='Viola 1 RH',
    rotation_indices=(0, 1, -1),
    secondary_voice_name='Viola 1 LH',
    secondary_music_specifier=consort.MusicSpecifierSequence(
        application_rate='phrase',
        music_specifiers=['two', 'three', 'four'],
        ),
    )
music_specifiers = {'Viola 1': composite_music_specifier}
timespan_inventory = timespan_maker(
    music_specifiers=music_specifiers,
    target_timespan=target_timespan,
    )
show(timespan_inventory, key='voice_name')
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> composite_music_specifier = consort.CompositeMusicSpecifier(
...     primary_music_specifier='one',
...     primary_voice_name='Viola 1 RH',
...     rotation_indices=(0, 1, -1),
...     secondary_voice_name='Viola 1 LH',
...     secondary_music_specifier=consort.MusicSpecifierSequence(
...         application_rate='phrase',
...         music_specifiers=['two', 'three', 'four'],
...         ),
...     )
>>> music_specifiers = {'Viola 1': composite_music_specifier}
>>> timespan_inventory = timespan_maker(
...     music_specifiers=music_specifiers,
...     target_timespan=target_timespan,
...     )
>>> show(timespan_inventory, key='voice_name')
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-8cc3626035e636ea504a8e38cdbb9e88.pdf}
\end{singlespacing}
\end{abjadbookoutput}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interpretation}
\label{sec:interpretation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

At any point during specification, a segment-maker may be interpreted to
produce an illustration. Score interpretation proceeds conceptually much like
compilation in classical computing, where a compiler parses an instruction set
written in some source language into an intermediate representation and then
transforms that same intermediate representation into instructions executable
on a target platform. In Consort's interpretation stage, the compiler is the
segment-maker itself, and the source instruction set its configuration -- its
tempo, permitted meters, music settings and so forth. Timespan inventories
produced by each music setting's timespan-maker, populated with timespans
annotated with music specifiers perform the role of the intermediate
representation. This intermediate representation acts as a \emph{maquette},
blocking out where in the resulting score segment various materials should be
deployed. The target of score interpretation is, unsurprisingly, a fully-fledge
score aggregated from Abjad score components. Interpretation takes place in two
broad stages -- rhythmic interpretation, followed by non-rhythmic
interpretation -- with the first stage producing a score populated solely with
rhythmic information, and the second stage applying grace notes, pitches,
indicators, spanners and various typographic overrides to the
previously-constructed rhythmic skeleton.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rhythmic interpretation}
\label{sec:rhythmic-interpretation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Broadly speaking, rhythmic interpretation proceeds from coarse- to
fine-grained. The segment-maker creates a *maquette* -- a model of the
locations of musical materials in the score -- by calling each of its
music-settings in turn to populate a timespan inventory. It then resolves
overlap conflicts within that inventory, fits meters against the resolved
inventory's offsets, splits and prunes the contents of the inventory according
to its fitted metrical structure, and finally converts the finished timespan
maquette into an actual score. This process, like interpretation overall, can
be roughly divided into work flows of \emph{maquette creation} and \emph{music
creation}, although in practice the two flows are interleaved significantly as
they actually influence one another. When creating the maquette, music settings
with \emph{independent} timespan-makers -- those which do not depend on the
contents of a previously created timespan inventory, specifically flooded and
talea timespan-makers -- are called in a first pass, and those with {dependent}
timespan-makers in a second. These two passes only differ significantly in that
meters are fitted against the segment's timespan maquette during the
independent timespan-maker pass, but not during the dependent.

\subsection{Populating the maquette}
\label{ssec:populating-the-maquette}

To populate the maquette, the segment-maker calls each of its music settings to
produce timespans according to their configured timespan-makers,
\emph{timespan-identifiers} -- optional specifications of which portion of the
segment's overall timespan to operate within -- and voice-associated music
specifiers. Timespan identifiers may include timespans, inventories of
timespans, or even expressions callable against the segment-makers own timespan
which evaluate to an inventory of timespans.

Music settings exist without any reference to a segment-maker, its desired
duration -- and therefore desired timespan --, or its score template. In order
to know which target timespan or timespans a music setting's timespan-maker
should operate within -- in the case of procedural timespan identifiers such as
\emph{ratio-parts expressions} which must be called against a preexisting
timespan in order to determine what part or parts of that timespan to use --
the music setting must resolve its timespan identifier against the segment's
desired duration. Target timespan resolution must also take into account offset
quantization, as the target timespans resulting from the evaluation of a
ratio-parts expression may not align against a power-of-two-denominator offset
grid such as \sfrac[big]{1}{8}, \sfrac[big]{1}{16} or \sfrac[big]{1}{32}. Because
timespan-makers produce their output relative to the start-offset of their
target timespan, a misaligned target timespan -- starting at an offset like
\sfrac[big]{1}{3} or \sfrac[big]{15}{7} rather than \sfrac[big]{1}{4} or 0 -- will cause all
generated timespans to be misaligned.

Music setting's associate their music specifiers with strings containing
voice-name abbreviations. These abbreviations are always underscore-delimited
strings such as \texttt{violin\_1} or \texttt{piano\_lh} -- necessitated by
Python's keyword argument syntax so that they can be used as keys during class
instantiation -- which represent voices in a score, without having established
a concrete reference to those voice contexts. In order to match its music
specifiers against actual voice contexts in a score, the music setting must
resolve its voice-name abbreviations against a score template, looking up each
abbreviation on the template and returning the real name of the associated
context. This lookup process allows music settings to construct well-formed
voice-name-to-music-specifier mappings, implemented as ordered dictionaries and
ordered by the actual *score index* -- effectively, the vertical location -- of
each looked-up context in the segment-maker's under-construction score. As
demonstrated in \autoref{sec:timespan-makers}, timespan-makers require these
mappings to produce their output. Additionally, voice-name resolution
guarantees that the values in the resolved voice-name-to-music-specifier
mapping are always either a \texttt{MusicSpecifierSequence} or
\texttt{CompositeMusicSpecifier} instance via coercion, where any composite
music-specifier's primary and secondary music specifiers are themselves coerced
into music specifier sequences. This coercion ensures that all arguments to the
music setting's timespan-maker are in a well-formed and predictable state.
\footnote{Timespan-makers actually delegate the creation of performed and
silent timespans to music specifier sequences. While not demonstrated
explicitly in \autoref{sec:timespan-makers}, this delegation allows
timespan-makers to use both music specifier sequences and composite music
specifiers interchangeably, with the former creating timespans associated with
one voice and the later with two. When the values of a timespan-maker's input
voice-name-to-music-specifier mapping are neither music specifier sequences nor
composite music specifiers -- as was the case in all of the examples in
\autoref{sec:timespan-makers} -- they implicitly coerce those values into music
specifier sequences.}

\begin{comment}
<abjad>
music_setting = consort.MusicSetting(
    timespan_identifier=consort.RatioPartsExpression(
        parts=(0, 2),
        ratio=(1, 3, 2),
        ),
    timespan_maker=consort.FloodedTimespanMaker(),
    violin_2_lh='A',
    viola_lh=('B', 'C', 'D'),
    cello=consort.CompositeMusicSpecifier(
        primary_music_specifier='one',
        secondary_music_specifier=consort.MusicSpecifierSequence(
            music_specifiers=['two', 'three', 'four'],
            ),
        )
    )
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_setting = consort.MusicSetting(
...     timespan_identifier=consort.RatioPartsExpression(
...         parts=(0, 2),
...         ratio=(1, 3, 2),
...         ),
...     timespan_maker=consort.FloodedTimespanMaker(),
...     violin_2_lh='A',
...     viola_lh=('B', 'C', 'D'),
...     cello=consort.CompositeMusicSpecifier(
...         primary_music_specifier='one',
...         secondary_music_specifier=consort.MusicSpecifierSequence(
...             music_specifiers=['two', 'three', 'four'],
...             ),
...         )
...     )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Consider the following string quartet score template, which will be
used with the above music setting. Note the names of the various contexts
defined in it, made visible when formatted as LilyPond syntax, where each
context name is given by a quoted string on the lines beginning with
\texttt{\textbackslash{}context}. The score contains a time signature context,
and four staff groups, one for each instrument in the quartet. These staff
groups then contain two staves, for the left and right hands of each
performer, with each staff containing a single voice:

\begin{comment}
<abjad>
score_template = consort.StringQuartetScoreTemplate()
score = score_template()
print(format(score))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> score_template = consort.StringQuartetScoreTemplate()
>>> score = score_template()
>>> print(format(score))
\context Score = "String Quartet Score" <<
    \tag #'time
    \context TimeSignatureContext = "Time Signature Context" {
    }
    \tag #'violin-1
    \context StringPerformerGroup = "Violin 1 Performer Group" \with {
        instrumentName = \markup {
            \hcenter-in
                #10
                "Violin 1"
            }
        shortInstrumentName = \markup {
            \hcenter-in
                #10
                "Vln. 1"
            }
    } <<
        \context BowingStaff = "Violin 1 Bowing Staff" {
            \clef "percussion"
            \context Voice = "Violin 1 Bowing Voice" {
            }
        }
        \context FingeringStaff = "Violin 1 Fingering Staff" {
            \clef "treble"
            \context Voice = "Violin 1 Fingering Voice" {
            }
        }
    >>
    \tag #'violin-2
    \context StringPerformerGroup = "Violin 2 Performer Group" \with {
        instrumentName = \markup {
            \hcenter-in
                #10
                "Violin 2"
            }
        shortInstrumentName = \markup {
            \hcenter-in
                #10
                "Vln. 2"
            }
    } <<
        \context BowingStaff = "Violin 2 Bowing Staff" {
            \clef "percussion"
            \context Voice = "Violin 2 Bowing Voice" {
            }
        }
        \context FingeringStaff = "Violin 2 Fingering Staff" {
            \clef "treble"
            \context Voice = "Violin 2 Fingering Voice" {
            }
        }
    >>
    \tag #'viola
    \context StringPerformerGroup = "Viola Performer Group" \with {
        instrumentName = \markup {
            \hcenter-in
                #10
                Viola
            }
        shortInstrumentName = \markup {
            \hcenter-in
                #10
                Va.
            }
    } <<
        \context BowingStaff = "Viola Bowing Staff" {
            \clef "percussion"
            \context Voice = "Viola Bowing Voice" {
            }
        }
        \context FingeringStaff = "Viola Fingering Staff" {
            \clef "alto"
            \context Voice = "Viola Fingering Voice" {
            }
        }
    >>
    \tag #'cello
    \context StringPerformerGroup = "Cello Performer Group" \with {
        instrumentName = \markup {
            \hcenter-in
                #10
                Cello
            }
        shortInstrumentName = \markup {
            \hcenter-in
                #10
                Vc.
            }
    } <<
        \context BowingStaff = "Cello Bowing Staff" {
            \clef "percussion"
            \context Voice = "Cello Bowing Voice" {
            }
        }
        \context FingeringStaff = "Cello Fingering Staff" {
            \clef "bass"
            \context Voice = "Cello Fingering Voice" {
            }
        }
    >>
>>
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Consort's score templates provide abbreviation-to-voice-name mappings
via their \texttt{context\_name\_abbreviations} property. Likewise, they
provide mappings for use with composite music specifiers which map
\enquote{parent} context abbreviations to their relevant child abbreviation
pairs:

\begin{comment}
<abjad>
for abbr, context_name in score_template.context_name_abbreviations.items():
    print(abbr, context_name)

for parent, child_pair in score_template.composite_context_pairs.items():
    print(parent, child_pair)

</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> for abbr, context_name in score_template.context_name_abbreviations.items():
...     print(abbr, context_name)
...
('viola_rh', 'Viola Bowing Voice')
('cello_rh', 'Cello Bowing Voice')
('violin_1_lh', 'Violin 1 Fingering Voice')
('violin_2_lh', 'Violin 2 Fingering Voice')
('violin_2_rh', 'Violin 2 Bowing Voice')
('cello', 'Cello Performer Group')
('viola', 'Viola Performer Group')
('violin_1_rh', 'Violin 1 Bowing Voice')
('viola_lh', 'Viola Fingering Voice')
('violin_2', 'Violin 2 Performer Group')
('cello_lh', 'Cello Fingering Voice')
('violin_1', 'Violin 1 Performer Group')
\end{minted}
\begin{minted}{pycon}
>>> for parent, child_pair in score_template.composite_context_pairs.items():
...     print(parent, child_pair)
...
('violin_2', ('violin_2_rh', 'violin_2_lh'))
('cello', ('cello_rh', 'cello_lh'))
('viola', ('viola_rh', 'viola_lh'))
('violin_1', ('violin_1_rh', 'violin_1_lh'))
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Resolving the previously defined music specifier against Consort's
string quartet score template dereferences the correct context names, allowing
them to be indexed into any score created by that score template. Note that not
only are the two non-composite music specifiers now associated with voices in
the score, but they have been recreated as music specifier sequences. Likewise,
the composite music specifier has been reconfigured such that it knows the
specific names of the two voices associated with its primary and secondary
music specifiers, themselves recreated as music specifier sequences:

\begin{comment}
<abjad>
result = music_setting.resolve_music_specifiers(score_template)
for context_name, resolved_music_specifier in result.items():
    print('CONTEXT:', context_name)
    print(format(resolved_music_specifier))

</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> result = music_setting.resolve_music_specifiers(score_template)
>>> for context_name, resolved_music_specifier in result.items():
...     print('CONTEXT:', context_name)
...     print(format(resolved_music_specifier))
...
('CONTEXT:', 'Violin 2 Fingering Voice')
consort.tools.MusicSpecifierSequence(
    music_specifiers=datastructuretools.CyclicTuple(
        ['A']
        ),
    )
('CONTEXT:', 'Viola Fingering Voice')
consort.tools.MusicSpecifierSequence(
    music_specifiers=datastructuretools.CyclicTuple(
        ['B', 'C', 'D']
        ),
    )
('CONTEXT:', 'Cello Performer Group')
consort.tools.CompositeMusicSpecifier(
    primary_music_specifier=consort.tools.MusicSpecifierSequence(
        music_specifiers=datastructuretools.CyclicTuple(
            ['one']
            ),
        ),
    primary_voice_name='Cello Bowing Voice',
    secondary_music_specifier=consort.tools.MusicSpecifierSequence(
        music_specifiers=datastructuretools.CyclicTuple(
            ['two', 'three', 'four']
            ),
        ),
    secondary_voice_name='Cello Fingering Voice',
    )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Resolving a music setting's timespan identifier against the
segment-maker's segment timespan results in one or more target timespans which
can be used as argument to the music setting's timespan-maker:

\begin{comment}
<abjad>
segment_timespan = timespantools.Timespan(0, 8)
show(segment_timespan)
target_timespans = music_setting.resolve_target_timespans(segment_timespan)
show(target_timespans, range_=(0, 8))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> segment_timespan = timespantools.Timespan(0, 8)
>>> show(segment_timespan)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-03868f10f80e7d1bf084b0902a4fbfa9.pdf}
\begin{minted}{pycon}
>>> target_timespans = music_setting.resolve_target_timespans(segment_timespan)
>>> show(target_timespans, range_=(0, 8))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-df77c2a528ec5ed55444c50521fad247.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Note that the start and stop offsets of the target timespans resolved
above do not all align at offsets with power-of-two denominators, such as
\sfrac[big]{1}{2}, \sfrac[big]{1}{4} or \sfrac[big]{1}{8}. By specifying a timespan
quantization, the target timespans generated during resolution can be quantized
to a grid:

\begin{comment}
<abjad>
target_timespans = music_setting.resolve_target_timespans(
    segment_timespan,
    timespan_quantization=Duration(1, 16),
    )
show(target_timespans, range_=(0, 8))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> target_timespans = music_setting.resolve_target_timespans(
...     segment_timespan,
...     timespan_quantization=Duration(1, 16),
...     )
>>> show(target_timespans, range_=(0, 8))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-5404f1777c60c602cce1e3c9cbcf07a7.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent A mask can also be applied to the target timespans:

\begin{comment}
<abjad>
music_setting = new(
    music_setting,
    timespan_identifier__mask_timespan=timespantools.Timespan(
        start_offset=(1, 2),
        stop_offset=7,
        ),
    )
target_timespans = music_setting.resolve_target_timespans(segment_timespan)
show(target_timespans, range_=(0, 8))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_setting = new(
...     music_setting,
...     timespan_identifier__mask_timespan=timespantools.Timespan(
...         start_offset=(1, 2),
...         stop_offset=7,
...         ),
...     )
>>> target_timespans = music_setting.resolve_target_timespans(segment_timespan)
>>> show(target_timespans, range_=(0, 8))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-98eae7430f8bab6f9c6f9dec7f5dfbe3.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Once resolved, each music setting can call its timespan-maker to
create timespans with the appropriate voice-name-to-music-specifier mapping,
target timespans and layer, adding the contents of the resulting inventory to
the growing maquette of performed and silent timespans produced by previous
music settings. The populating process repeats until no more music settings
remain.

\subsection{Resolving cascading overlap}
\label{ssec:resolving-cascading-overlap}

One of the driving motivations behind Consort is the ability to create musical
textures consisting of multiple overlapping layers, each created by an
independent maker and each with different materials from the other layers,
allowing multiple materials of various provenances to appear in the same
instrumental voice. Still, because acoustic instruments cannot simply
\enquote{create} arbitrary numbers of voices like a synthesizer might, any
overlap in material allocated for a given voice needs to be resolved. Consort
handles resolution of overlap via a tournament, choosing only one material from
a collection of overlapping candidates.

Consider the following three timespan inventories, created with three different
timespan-makers and music-specifier mappings. The first inventory is created
via a flooded timespan-maker, filling the entirety of a target timespan of
\sfrac[big]{0}{1} to \sfrac[big]{19}{4} in voices \enquote{Voice 2} and \enquote{Voice
3}. This inventory behaves like a constant \enquote{background layer} for those
two voices:

\begin{comment}
<abjad>
layer_1_timespan_maker = consort.FloodedTimespanMaker()
layer_1_target_timespan = timespantools.Timespan(0, (19, 4))
layer_1_music_specifiers = collections.OrderedDict([
    ('Voice 2', None),
    ('Voice 3', None),
    ])
layer_1 = layer_1_timespan_maker(
    layer=1,
    music_specifiers=layer_1_music_specifiers,
    target_timespan=layer_1_target_timespan,
    )
show(layer_1, key='voice_name', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> layer_1_timespan_maker = consort.FloodedTimespanMaker()
>>> layer_1_target_timespan = timespantools.Timespan(0, (19, 4))
>>> layer_1_music_specifiers = collections.OrderedDict([
...     ('Voice 2', None),
...     ('Voice 3', None),
...     ])
>>> layer_1 = layer_1_timespan_maker(
...     layer=1,
...     music_specifiers=layer_1_music_specifiers,
...     target_timespan=layer_1_target_timespan,
...     )
>>> show(layer_1, key='voice_name', range_=(0, (21, 4)))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-e32bab64f031f3deb1809e4b12eaeb5e.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent The second timespan inventory is created by a talea timespan-maker.
This inventory covers all four voices -- \enquote{Voice 1}, \enquote{Voice 2},
\enquote{Voice 3} and \enquote{Voice 4} -- with a texture of evenly distributed
phrases and silences. However, unlike the first timespan inventory, this
texture only spans the target timespan of \sfrac[big]{3}{4} to \sfrac[big]{19}{4},
guaranteeing that the background layer for \enquote{Voice 2} and \enquote{Voice
3} is untouched in the span of \sfrac[big]{0}{1} to \sfrac[big]{3}{4}:

\begin{comment}
<abjad>
layer_2_timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea(
        counts=(0, 1, 3),
        denominator=8,
        ),
    playing_groupings=(1, 2),
    playing_talea=rhythmmakertools.Talea(
        counts=(1, 2, 3, 4),
        denominator=4,
        ),
    silence_talea=rhythmmakertools.Talea(
        counts=(5, 3, 1),
        denominator=8,
        ),
    )
layer_2_target_timespan = timespantools.Timespan((3, 4), (19, 4))
layer_2_music_specifiers = collections.OrderedDict([
    ('Voice 1', None),
    ('Voice 2', None),
    ('Voice 3', None),
    ('Voice 4', None),
    ])
layer_2 = layer_2_timespan_maker(
    layer=2,
    music_specifiers=layer_2_music_specifiers,
    target_timespan=layer_2_target_timespan,
    )
show(layer_2, key='voice_name', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> layer_2_timespan_maker = consort.TaleaTimespanMaker(
...     initial_silence_talea=rhythmmakertools.Talea(
...         counts=(0, 1, 3),
...         denominator=8,
...         ),
...     playing_groupings=(1, 2),
...     playing_talea=rhythmmakertools.Talea(
...         counts=(1, 2, 3, 4),
...         denominator=4,
...         ),
...     silence_talea=rhythmmakertools.Talea(
...         counts=(5, 3, 1),
...         denominator=8,
...         ),
...     )
>>> layer_2_target_timespan = timespantools.Timespan((3, 4), (19, 4))
>>> layer_2_music_specifiers = collections.OrderedDict([
...     ('Voice 1', None),
...     ('Voice 2', None),
...     ('Voice 3', None),
...     ('Voice 4', None),
...     ])
>>> layer_2 = layer_2_timespan_maker(
...     layer=2,
...     music_specifiers=layer_2_music_specifiers,
...     target_timespan=layer_2_target_timespan,
...     )
>>> show(layer_2, key='voice_name', range_=(0, (21, 4)))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-6b6bd4f0c5bd1a898f925cb1084d6590.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent The third timespan inventory consists of groups of near-simultaneous
attacks in three voices -- \enquote{Voice 1}, \enquote{Voice 3} and
\enquote{Voice 4}. This inventory's talea timespan-maker has padded
\sfrac[big]{1}{4}-duration silences around the beginning and end of each group,
guaranteeing that any performed timespans with layers lower than 3 will be
masked not only by this layer's performed timespans, but by its silent
timespans as well. Additionally, the third timespan inventory was created with
a target timespan of \sfrac[big]{6}{4} to \sfrac[big]{21}{4}. While -- due to
the complexities of the talea timespan-maker's patterns -- the generated
timespan texture may not extend all of the way to its target timespan's stop
offset at \sfrac[big]{21}{4}, it will certainly not contain any performed or
silent timespans earlier than \sfrac[big]{5}{4} -- the target timespan's start
offset minus the timespan-maker's padding --, leaving lower layers untouched
from \sfrac[big]{0}{1} to \sfrac[big]{5}{4}:

\begin{comment}
<abjad>
layer_3_timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea(
        counts=(0, 0, 0, 1),
        denominator=8,
        ),
    padding=Duration(1, 4),
    playing_talea=rhythmmakertools.Talea(
        counts=(2, 3, 4),
        denominator=8,
        ),
    silence_talea=rhythmmakertools.Talea(
        counts=(6,),
        denominator=4,
        ),
    synchronize_step=True,
    )
layer_3_target_timespan = timespantools.Timespan((6, 4), (21, 4))
layer_3_music_specifiers = collections.OrderedDict([
    ('Voice 1', None),
    ('Voice 3', None),
    ('Voice 4', None),
    ])
layer_3 = layer_3_timespan_maker(
    layer=3,
    music_specifiers=layer_3_music_specifiers,
    target_timespan=layer_3_target_timespan,
    )
show(layer_3, key='voice_name', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> layer_3_timespan_maker = consort.TaleaTimespanMaker(
...     initial_silence_talea=rhythmmakertools.Talea(
...         counts=(0, 0, 0, 1),
...         denominator=8,
...         ),
...     padding=Duration(1, 4),
...     playing_talea=rhythmmakertools.Talea(
...         counts=(2, 3, 4),
...         denominator=8,
...         ),
...     silence_talea=rhythmmakertools.Talea(
...         counts=(6,),
...         denominator=4,
...         ),
...     synchronize_step=True,
...     )
>>> layer_3_target_timespan = timespantools.Timespan((6, 4), (21, 4))
>>> layer_3_music_specifiers = collections.OrderedDict([
...     ('Voice 1', None),
...     ('Voice 3', None),
...     ('Voice 4', None),
...     ])
>>> layer_3 = layer_3_timespan_maker(
...     layer=3,
...     music_specifiers=layer_3_music_specifiers,
...     target_timespan=layer_3_target_timespan,
...     )
>>> show(layer_3, key='voice_name', range_=(0, (21, 4)))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-5b0efece0602ac261c04ef4466e1a290.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Recall from \autoref{sec:timespan-makers} that timespan-makers can
modify a timespan inventory in-place when called, rather than generating a new
one from scratch. The following code -- greatly simplified from Consort's
\texttt{SegmentMaker.populate\_multiplexed\_maquette()} method -- demonstrates
the process of calling multiple timespan-makers with their corresponding target
timespans and voice-name-to-music-specifier mappings to progressively populate
a single timespan inventory in-place. Note the use of two Python builtin
iterators \texttt{zip()} and \texttt{enumerate()}. The \texttt{zip()} iterators
iterates over the iterables with which it was instantiated yielding the first
item of each of its iterables as a tuple, then the second of each of its
iterables, then the third, and so forth. The \texttt{enumerate()} iterator
yields each item of its input iterable paired with that item's index, filling
the role in Python for the verbose \emph{for loop} loop idiom found in many
C-like languages, such as Java or Javascript: \texttt{for(int x = 10; x < 20; x
= x + 1) \{ ... \}}.

\begin{comment}
<abjad>
timespan_makers = (
    layer_1_timespan_maker,
    layer_2_timespan_maker,
    layer_3_timespan_maker,
    )
music_specifiers = (
    layer_1_music_specifiers,
    layer_2_music_specifiers,
    layer_3_music_specifiers,
    )
target_timespans = (
    layer_1_target_timespan,
    layer_2_target_timespan,
    layer_3_target_timespan,
    )
triples = zip(timespan_makers, music_specifiers, target_timespans)
timespan_inventory = timespantools.TimespanInventory()
for layer, triple in enumerate(triples, 1):
    timespan_inventory = triple[0](
        layer=layer,
        music_specifiers=triple[1],
        target_timespan=triple[2],
        timespan_inventory=timespan_inventory,
        )

</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> timespan_makers = (
...     layer_1_timespan_maker,
...     layer_2_timespan_maker,
...     layer_3_timespan_maker,
...     )
>>> music_specifiers = (
...     layer_1_music_specifiers,
...     layer_2_music_specifiers,
...     layer_3_music_specifiers,
...     )
>>> target_timespans = (
...     layer_1_target_timespan,
...     layer_2_target_timespan,
...     layer_3_target_timespan,
...     )
>>> triples = zip(timespan_makers, music_specifiers, target_timespans)
>>> timespan_inventory = timespantools.TimespanInventory()
>>> for layer, triple in enumerate(triples, 1):
...     timespan_inventory = triple[0](
...         layer=layer,
...         music_specifiers=triple[1],
...         target_timespan=triple[2],
...         timespan_inventory=timespan_inventory,
...         )
...
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Recall that the result of this process is still a timespan inventory,
just as described in \autoref{sec:timespan-inventories}. While this document
has taken pains to clarify the internal structure of timespan-maker-generated
timespan inventories by sorting and displaying voice-names in their
illustrations -- just as in the above three timespan inventory illustrations --
that behavior derives from the timespan inventory's illustration protocol
implementation, and does not reflect their actual, flat structure:

\begin{comment}
<abjad>
show(timespan_inventory, range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> show(timespan_inventory, range_=(0, (21, 4)))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-5411fdd503f490d58096495d804cd218.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Just because timespan inventories are often displayed with
voice-names, and contain performed timespans with voice-name attributes, does
not mean that they are automatically or internally structured that way. Such
sorting requires an additional pass -- demultiplexing. Visualizing the
inventory, exploded by voice-name, then sorted by layer, shows the overlap in
each voice:

\begin{comment}
<abjad>
show(timespan_inventory, key='voice_name', sortkey='layer', range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> show(timespan_inventory, key='voice_name', sortkey='layer', range_=(0, (21, 4)))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-8ce5e47ebdb40eb18af31017b0444226.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent In order to resolve cascading overlap, the segment-maker must first
demultiplex the performed and silent timespans in the still-multiplexed
maquette into separate timespan inventories by their voice-name attributes. The
segment-maker further separates each demultiplexed-by-voice-name timespan
inventory into multiple timespan inventories according to their contents' layer
attributes, with the lowest-layered inventory first, and the highest-layered
inventory last. This results in one inventory per-voice, per-timespan-maker
from the maquette population process.\footnote{Why not keep all timespan-maker
output separated from the very beginning? Working with a single multiplexed
timespan inventory for much of the rhythmic interpretation process simplifies
many of the procedures used therein, such as dependent timespan-maker
evaluation, splitting, consolidation, etc. Compared to many of the later
rhythmic operations, such as meter rewriting, multiplexing and demultiplexing
timespan inventories are computationally trivial.} With the maquette fully
demultiplexed, the segment-maker can proceed through each layer in each voice,
from lowest to highest. It progressively subtracts the timespans in each higher
inventory from the lowest inventory, effectively cutting out holes outlining
the shapes of that higher inventory's timespans, then adds that higher
inventory's timespans into the lowest-layered inventory. This process masks
lower-layered timespans with higher ones. The process repeats until no more
timespan inventories remain for that voice, then moves onto the inventories for
the next voice. The resolved, demultiplexed results are finally collected into
a timespan inventory mapping, associating voice names with resolved timespan
inventories, and returned.

\begin{comment}
<abjad>
demultiplexed_maquette = consort.SegmentMaker.resolve_maquette(
    timespan_inventory,
    )
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> demultiplexed_maquette = consort.SegmentMaker.resolve_maquette(
...     timespan_inventory,
...     )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent After resolution, no overlap remains in the timespans for any voice.
Note too that no silent timespans -- like those created as padding in the third
timespan-maker above -- remain either. Silent timespans act solely as a means
of \enquote{holding space} for a layer, masking but not replacing timespans in
lower layers:

\begin{comment}
<abjad>
show(demultiplexed_maquette, range_=(0, (21, 4)))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> show(demultiplexed_maquette, range_=(0, (21, 4)))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-4333255394732195d4d76c7942b9a767.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Unlike many of the timespan-handling functions demonstrated in this
chapter as well as in \autoref{chap:time-tools}, \texttt{resolve\_maquette()}
returns a \texttt{TimespanInventoryMapping} rather than a
\texttt{TimespanInventory}. The timespan inventory mapping already
explicitly uses voice-names as keys, obviating the need for a
\texttt{key='voice\_name'} keyword argument pair in the call to
\texttt{show()}.

\subsection{Finding meters, revisited}
\label{ssec:finding-meters-revisited}

Consort's segment-maker implements a variation on the meter-fitting algorithm
described in \autoref{sec:finding-meters}. Each segment-maker may be configured
with an inventory of permitted meters, as well as maximum meter run length, in
order to drive the meter fitting algorithm. When counting offsets,
segment-makers include the offsets found on the performed timespans in their
maquette but discard those from silent timespans, removing any influence from
timespans created solely for silencing other timespans. The start offset of
each performed timespan is weighed twice as much as their stop offset. This
imbalance helps emphasize simultaneous phrase starts across different voices.
Additionally, segment-maker's weight their own desired stop offset at a much
higher value than any count derived from the offsets in their maquette. This
attempts to influence the meter fitting process into selecting a series of
meters which end as close to their desired stop offset as possible. After
fitting meters, the segment-maker caches both the fitted meters and their
boundaries as properties on its instance, affording later retrieval by other
subroutines.

\subsection{Splitting, pruning \& consolidation}
\label{ssec:splitting-pruning-and-consolidation}

Once meters have been fitted against the resolved maquette, the timespans in
the maquette must be split at the measure boundaries outlined by those meters.
Splitting guarantees that no timespans cross any bar-lines and that therefore
no containers generated by those timespans when notating them as score
components cross any bar-lines either. While LilyPond can typeset bar-line
crossing notes, chords and even tuplets, the scores I have composed via Consort
do not currently make use of such constructions. As described in
\autoref{ssec:operations-on-timespans}, operations on timespans which change
offsets -- generating new timespans in the process, rather than modifying the
operated-upon timespan in-place -- preserve their unmodified properties via
templating. Splitting is no exception, and those timespans split maintain their
music specifiers, layer identifiers and voice-names:

\begin{comment}
<abjad>
performed_timespan = consort.PerformedTimespan(
    layer=3,
    start_offset=(1, 2),
    stop_offset=(13, 8),
    voice_name='Percussion Voice',
    )
shards = performed_timespan.split_at_offset((9, 16))
print(format(shards))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> performed_timespan = consort.PerformedTimespan(
...     layer=3,
...     start_offset=(1, 2),
...     stop_offset=(13, 8),
...     voice_name='Percussion Voice',
...     )
>>> shards = performed_timespan.split_at_offset((9, 16))
>>> print(format(shards))
timespantools.TimespanInventory(
    [
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(1, 2),
            stop_offset=durationtools.Offset(9, 16),
            layer=3,
            original_stop_offset=durationtools.Offset(13, 8),
            voice_name='Percussion Voice',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(9, 16),
            stop_offset=durationtools.Offset(13, 8),
            layer=3,
            original_start_offset=durationtools.Offset(1, 2),
            voice_name='Percussion Voice',
            ),
        ]
    )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent After splitting, the segment-maker prunes timespans considered either
too short or malformed. Performed timespans may be configured with a
\texttt{minimum\_duration} property. Timespan-makers may set this property on
timespans they create when they are themselves configured with a
\texttt{TimespanSpecifier}. Any performed timespan whose actual duration is
less than its minimum duration -- if it has been configured with a minimum
duration -- will be removed from the maquette. Likewise any timespan with a
duration of 0 -- therefore malformed -- will also be removed. While the latter
pruning guarantees correctness of the maquette -- malformed timespans cannot be
rendered as notation at all, and may cause other problems when partitioning due
to ambiguities in their start / stop offset semantics --, the former allows for
a kind of compositional control over the maquette. When notated with certain
rhythm-makers, overly short divisions -- especially those shorter than
\sfrac[big]{1}{8}-duration -- may give undesirable results. Note that silent
timespans have no configurable minimum duration. Their
\texttt{minimum\_duration} always returns 0. They maintain this dummy property
so that the segment-maker's timespan-pruning algorithms can treat silent and
performed timespans identically.

Next, Consort's segment-maker \emph{consolidates} contiguous performed
timespans with identical music specifiers, caching the durations of the
consolidated timespans in a new timespan's \texttt{divisions} property. Each
new consolidated timespan outlines the start and stop offset of its
consolidated group:

\begin{comment}
<abjad>
timespans = timespantools.TimespanInventory([
    consort.PerformedTimespan(
        start_offset=0,
        stop_offset=10,
        music_specifier='foo',
        ),
    consort.PerformedTimespan(
        start_offset=10,
        stop_offset=20,
        music_specifier='foo',
        ),
    consort.PerformedTimespan(
        start_offset=20,
        stop_offset=25,
        music_specifier='bar',
        ),
    consort.PerformedTimespan(
        start_offset=40,
        stop_offset=50,
        music_specifier='bar',
        ),
    consort.PerformedTimespan(
        start_offset=50,
        stop_offset=58,
        music_specifier='bar',
        ),
    ])
show(timespans)
consolidated_timespans = consort.SegmentMaker.consolidate_timespans(timespans)
show(consolidated_timespans)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> timespans = timespantools.TimespanInventory([
...     consort.PerformedTimespan(
...         start_offset=0,
...         stop_offset=10,
...         music_specifier='foo',
...         ),
...     consort.PerformedTimespan(
...         start_offset=10,
...         stop_offset=20,
...         music_specifier='foo',
...         ),
...     consort.PerformedTimespan(
...         start_offset=20,
...         stop_offset=25,
...         music_specifier='bar',
...         ),
...     consort.PerformedTimespan(
...         start_offset=40,
...         stop_offset=50,
...         music_specifier='bar',
...         ),
...     consort.PerformedTimespan(
...         start_offset=50,
...         stop_offset=58,
...         music_specifier='bar',
...         ),
...     ])
>>> show(timespans)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-99903dd7d5e367134ef359e882f910a5.pdf}
\begin{minted}{pycon}
>>> consolidated_timespans = consort.SegmentMaker.consolidate_timespans(timespans)
>>> show(consolidated_timespans)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-b20210ea4445f4d7344052372c626f62.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Consolidation transforms performed timespans from free-floating cells
in the maquette into components of larger phrases. The cached divisions also
prepare these consolidated timespans for \emph{inscription} by defining the
correct input for a rhythm-maker: a sequence of divisions.

\begin{comment}
<abjad>
print(format(consolidated_timespans))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> print(format(consolidated_timespans))
timespantools.TimespanInventory(
    [
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(0, 1),
            stop_offset=durationtools.Offset(20, 1),
            divisions=(
                durationtools.Duration(10, 1),
                durationtools.Duration(10, 1),
                ),
            music_specifier='foo',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(20, 1),
            stop_offset=durationtools.Offset(25, 1),
            divisions=(
                durationtools.Duration(5, 1),
                ),
            music_specifier='bar',
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(40, 1),
            stop_offset=durationtools.Offset(58, 1),
            divisions=(
                durationtools.Duration(10, 1),
                durationtools.Duration(8, 1),
                ),
            music_specifier='bar',
            ),
        ]
    )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent If the music specifier of the consolidated timespan was configured
with a \emph{minimum phrase duration}, and the consolidated timespan falls
under that threshold, it too is discarded.

\subsection{Inscription}
\label{ssec:inscription}

\emph{Inscription} describes the process of generating \emph{music} from a
performed timespan's divisions and rhythm-maker and \emph{inscribing} the
timespan with the result. Consort's segment-maker performs inscription by
iterating over the timespans for each voice in the demultiplexed maquette, in
score order. For each performed timespan encountered, the segment-maker
retrieves that performed timespan's music specifier, and increments that music
specifiers count in a counter. This allows each music specifier to maintain a
seed value while inscribing each performed timespan, even in fragmentary
textures, and to produce continuously varied results from each successive
rhythm-maker belonging to the same music specifier. Recall from
\autoref{ssec:populating-voices} that rhythm-makers can be called not only with
a list of divisions, but also a seed value, rotating the rhythm-makers
sequence-like properties when creating its rhythmic output. All of the handlers
discussed in \autoref{sec:non-rhythmic-interpretation} employ similar -- or
even more complex -- techniques for maintaining state across different phrases
sharing the same music specifier.
The segment-maker also retrieves the performed timespan's division list --
created during consolidation, as described in
\autoref{ssec:splitting-pruning-and-consolidation} -- and its rhythm-maker.
Rhythm-maker retrieval, like seed retrieval, is non-trivial. A performed
timespan's music specifier may not have rhythm-maker defined, or that performed
timespan may not even have a music specifier defined. If a performed timespan
\emph{has} a rhythm-maker defined on its music specifier, the segment-maker
retrieves that. If the timespan has a music specifier, but no defined
rhythm-maker, the segment-maker constructs a note rhythm-maker which ties all
of its divisions together. If the timespan has no music specifier defined at
all, the segment-maker returns a fully-masked note rhythm-maker.

With the performed timespan's seed, rhythm-maker and division list ready, the
segment-maker creates the performed timespan's music. This proceeds almost
identically to the \texttt{make\_music()} function described in
\autoref{ssec:populating-voices}. Consort's segment-maker makes one additional
adjustment on top of that algorithm, replacing trivially-prolated tuplets --
tuplets with ratios of 1:1 -- with unprolated containers. Next, the
segment-maker performs \emph{rest consolidation} on the newly-generated phrase,
grouping all of the phrase's unprolated rests -- those not appearing in tuplets
-- into their own containers, and leaving all other components -- all notes and
chords, and any rests found within a tuplet -- in their original division
within the phrase. Rest consolidation allows the segment-maker to not only
regroup the contents of a phrase into silent and non-silent segments, but to
actually split the performed timespan itself, creating larger gaps within the
maquette, and improving the chances for notating full-bar rests when finally
filling in silences between phrases.

Consider the following phrase-like container -- annotated to show its internal
division structure --, containing divisions in various configurations --
with rests at the beginning, at the end, with prolated rests, no rests at all,
and so forth:

\begin{comment}
<abjad>
parseable = r'''
\new Voice {
    {
        { \time 4/4 r4 c4 }
        { \times 2/3 { c4 r8 } r4 }
        { \time 4/4 r4 c4. r8 }
        { r4 \break }
        { \time 3/4 r4 }
        { c8 c4. }
        \times 2/3 { \time 2/4 r4 c4 c4 }
        { \time 4/4 c4. r8 }
        { r8 c4 r8 }
    }
}
'''
unconsolidated_staff = Staff(parseable, context_name='RhythmicStaff')
consort.annotate(unconsolidated_staff)
show(unconsolidated_staff)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> parseable = r'''
... \new Voice {
...     {
...         { \time 4/4 r4 c4 }
...         { \times 2/3 { c4 r8 } r4 }
...         { \time 4/4 r4 c4. r8 }
...         { r4 \break }
...         { \time 3/4 r4 }
...         { c8 c4. }
...         \times 2/3 { \time 2/4 r4 c4 c4 }
...         { \time 4/4 c4. r8 }
...         { r8 c4 r8 }
...     }
... }
... '''
>>> unconsolidated_staff = Staff(parseable, context_name='RhythmicStaff')
>>> consort.annotate(unconsolidated_staff)
>>> show(unconsolidated_staff)
\end{minted}
\noindent\includegraphics[max width=\textwidth,page=1]{assets/lilypond-10dd7b5abcf5b9106449153c363d2c73.pdf}
\newline
\newline
\noindent\includegraphics[max width=\textwidth,page=2]{assets/lilypond-10dd7b5abcf5b9106449153c363d2c73.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent A clear comparison can be made by duplicating the original phrase,
consolidating its rests, annotating it, and grouping both the original
unconsolidated phrase and the consolidated into a staff group, with the
original above and the altered below:

\begin{comment}
<abjad>
consolidated_staff = Staff(parseable, context_name='RhythmicStaff')
for voice in consolidated_staff:
    for phrase in voice:
        phrase = consort.SegmentMaker.consolidate_rests(phrase)

consort.annotate(consolidated_staff)
staff_group = StaffGroup([unconsolidated_staff, consolidated_staff])
show(staff_group)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> consolidated_staff = Staff(parseable, context_name='RhythmicStaff')
>>> for voice in consolidated_staff:
...     for phrase in voice:
...         phrase = consort.SegmentMaker.consolidate_rests(phrase)
...
>>> consort.annotate(consolidated_staff)
>>> staff_group = StaffGroup([unconsolidated_staff, consolidated_staff])
>>> show(staff_group)
\end{minted}
\noindent\includegraphics[max width=\textwidth,page=1]{assets/lilypond-1cd22521bb9d39ce643e21e7ec4ec133.pdf}
\newline
\newline
\noindent\includegraphics[max width=\textwidth,page=2]{assets/lilypond-1cd22521bb9d39ce643e21e7ec4ec133.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Note above how after rest consolidation the rests in the lower staff
have been grouped together into their own divisions -- annotated with dashed
lines -- while all other components -- including rests within tuplets -- remain
in their original divisions. Now consider the following simple rhythm-maker
which, when passed a division sequence of seven \sfrac[big]{1}{4}-durations,
generates a rhythm consisting of two groups of \sfrac[big]{1}{4} notes delimited by
rests:

\begin{comment}
<abjad>
rhythm_maker=rhythmmakertools.NoteRhythmMaker(
    output_masks=[
        rhythmmakertools.BooleanPattern(
            indices=[0],
            period=3,
            ),
        ],
    )
divisions = [Duration(1, 4)] * 7
show(rhythm_maker, divisions=divisions)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> rhythm_maker=rhythmmakertools.NoteRhythmMaker(
...     output_masks=[
...         rhythmmakertools.BooleanPattern(
...             indices=[0],
...             period=3,
...             ),
...         ],
...     )
>>> divisions = [Duration(1, 4)] * 7
>>> show(rhythm_maker, divisions=divisions)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-41db2ebb806422340ff52eeaef4859ca.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent The above rhythm-maker can be treated as the rhythm-maker for a music
specifier annotating a performed timespan. Likewise, the divisions used above
can be used as the division sequence for this same performed timespan. If this
timespan's rhythm-maker would be called with its division list, the same rhythm
as above would result:

\begin{comment}
<abjad>
timespan = consort.PerformedTimespan(
    divisions=divisions,
    start_offset=0,
    stop_offset=(7, 4),
    music_specifier=consort.MusicSpecifier(
        rhythm_maker=rhythm_maker,
        ),
    )
show(timespan)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> timespan = consort.PerformedTimespan(
...     divisions=divisions,
...     start_offset=0,
...     stop_offset=(7, 4),
...     music_specifier=consort.MusicSpecifier(
...         rhythm_maker=rhythm_maker,
...         ),
...     )
>>> show(timespan)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-1dcef40388b10322ae916d9a3a7a43b2.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent The timespan can then be inscribed by calling the segment-maker's
\texttt{inscribe\_timespan()} method:

\begin{comment}
<abjad>
inscribed_timespans = consort.SegmentMaker.inscribe_timespan(timespan)
show(inscribed_timespans, range_=(0, (7, 4)))
print(format(inscribed_timespans))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> inscribed_timespans = consort.SegmentMaker.inscribe_timespan(timespan)
>>> show(inscribed_timespans, range_=(0, (7, 4)))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-79bb9b9c4b4601a356cefa1f93444ba3.pdf}
\begin{minted}{pycon}
>>> print(format(inscribed_timespans))
timespantools.TimespanInventory(
    [
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(1, 4),
            stop_offset=durationtools.Offset(3, 4),
            music=scoretools.Container(
                "{   c'4 } {   c'4 }"
                ),
            music_specifier=consort.tools.MusicSpecifier(
                rhythm_maker=rhythmmakertools.NoteRhythmMaker(
                    output_masks=rhythmmakertools.BooleanPatternInventory(
                        (
                            rhythmmakertools.BooleanPattern(
                                indices=(0,),
                                period=3,
                                ),
                            )
                        ),
                    ),
                ),
            original_start_offset=durationtools.Offset(0, 1),
            original_stop_offset=durationtools.Offset(7, 4),
            ),
        consort.tools.PerformedTimespan(
            start_offset=durationtools.Offset(1, 1),
            stop_offset=durationtools.Offset(3, 2),
            music=scoretools.Container(
                "{   c'4 } {   c'4 }"
                ),
            music_specifier=consort.tools.MusicSpecifier(
                rhythm_maker=rhythmmakertools.NoteRhythmMaker(
                    output_masks=rhythmmakertools.BooleanPatternInventory(
                        (
                            rhythmmakertools.BooleanPattern(
                                indices=(0,),
                                period=3,
                                ),
                            )
                        ),
                    ),
                ),
            original_start_offset=durationtools.Offset(0, 1),
            original_stop_offset=durationtools.Offset(7, 4),
            ),
        ]
    )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent After inscription -- including rest consolidation --, the original
performed timespan has been split into two new performed timespans, whose start
and stop offsets outline only those portions of the generated rhythm which do
not contain rests. Their music attributes have likewise been populated with
only the non-silent portions of that rhythm. During inscription, the
segment-maker replaces each performed timespan against which inscription is
called with the result of the inscription process. Additionally, the
segment-maker performs some simple post-processing on each inscribed timespan's
music, attaching both a beam spanner to the leaves of the phrase, as well as
the music specifier used to specify the phrase's music to the phrase itself:

\begin{comment}
<abjad>
music = inscribed_timespans[0].music
indicator = inspect_(music).get_indicator(consort.MusicSpecifier)
print(format(indicator))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music = inscribed_timespans[0].music
>>> indicator = inspect_(music).get_indicator(consort.MusicSpecifier)
>>> print(format(indicator))
consort.tools.MusicSpecifier(
    rhythm_maker=rhythmmakertools.NoteRhythmMaker(
        output_masks=rhythmmakertools.BooleanPatternInventory(
            (
                rhythmmakertools.BooleanPattern(
                    indices=(0,),
                    period=3,
                    ),
                )
            ),
        ),
    )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Attaching the music specifier directly to the performed timespan's
phrase container allows each leaf in the segment-maker's score to locate the
music specifier which specified, without needing to resort to referencing the
maquette.

\subsection{Meter pruning}
\label{ssec:meter-pruning}

After the timespan pruning outlined in
\autoref{ssec:splitting-pruning-and-consolidation}, and the possibility of gaps
introduced due to rest consolidation as outlined in \autoref{ssec:inscription},
the overall stop offset of the maquette -- not the stop offset derived from the
segment-maker's desired duration -- may have shifted earlier. Depending on the
degree of shift, timespans in the maquette may no longer occur during one or
more of the implicit timespans of the previously fitted meters. Segment-makers
may be configured to discard these silences via their
\texttt{discard\_final\_silence} property, progressively removing meters from
the end of the list of fitted meters until one overlaps at least one performed
timespan in the maquette.

\subsection{Populating dependent timespans}
\label{ssec:populating-dependent-timespans}

The previous few passages, from \autoref{ssec:populating-the-maquette} through
\autoref*{ssec:meter-pruning}, describe the process of populating a
segment-makers's timespan maquette with the products of its *independent* music
settings -- those music settings whose timespan makers are independent, notably
flooded and talea timespan-makers. With the maquette partially populated, those
music settings with dependent timespan-makers -- timespan-makers which generate
timespans based on the contents of a preexisting timespan inventory -- may
finally be called to provide their contributions to the maquette. Dependent
population proceeds almost identically to independent population with a few
notable differences. For one, dependent population dispenses with meter finding
entirely. Consort treats meter as entirely dependent upon independent
timespans, as dependent timespans -- in practice -- are generally used for
keyboard pedaling voices, and should therefore have little bearing on the
overall metrical structure. And while maquette resolving, as described in
\autoref{ssec:resolving-cascading-overlap}, results in a demultiplexed timespan
inventory mapping -- a dictionary of voice-names to timespan inventories --,
independent timespan population completes by \emph{re-multiplexing} that
mapping -- effectively flattening -- back into a single timespan inventory.
This flattening prepares the correct input for dependent timespan population,
as dependent timespans require a single pre-populated timespan inventory as
input. As there are no more passes of timespans to add after dependent timespan
population -- discounting the population of silent timespans, as described in
\autoref{ssec:populating-silent-timespans} -- dependent timespan population
completes with its maquette still demultiplexed.

\subsection{Populating silent timespans}
\label{ssec:populating-silent-timespans}

With the maquette finally populated by inscribed performed timespans, properly
split, resolved and consolidated, the segment-maker can fill in the remaining
gaps between phrases in each voice. This is accomplished by creating
rest-inscribed timespans for each gap. As there are no more layers to add to
the maquette, the segment-maker can populate, split and inscribe timespans for
each of these gaps in a single pass. For each voice in the segment-maker's
still-empty score, the segment-maker retrieves all timespans -- if any --
associated with that voice and subtracts each of them in turn from a single
silent timespan the length of the entire segment, as determined by the first
and last meter boundaries. Any remaining shards from that segment-length silent
timespan represent gaps between phrases in that voice. If the maquette contains
no performed timespans associated with that voice, the segment-length silent
timespan remains unaltered. The segment-maker then splits the remaining silent
timespan shards at every intersecting meter boundary, as described in
\autoref{ssec:splitting-pruning-and-consolidation}, guaranteeing that the
resulting silent timespans do not cross bar-lines. Once split, the
segment-maker partitions the silent timespans, and iterates over the
partitioned groups. For each group of contiguous silent timespans, it generates
a phrase of music containing only rests using a fully-masked note rhythm-maker
-- as described in \autoref{ssec:inscription} --, attaches a dummy
music-specifier to the phrase and instantiates a performed timespan annotated
with that phrase, adding it to the current voice's timespan inventory in the
demultiplex maquette.

Recall the demultiplexed maquette created earlier in
\autoref{ssec:resolving-cascading-overlap}:

\begin{comment}
<abjad>
show(demultiplexed_maquette, range_=(0, 6))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> show(demultiplexed_maquette, range_=(0, 6))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-982613e6098f3c0e7eb8e12d525f59e3.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent We can simulate the process of populating silent timespans in the
following code, creating silent timespans -- purely for visualization purposes
-- rather than the rest-inscribed timespans actually created during silent
timespan population:

\begin{comment}
<abjad>
segment_duration = Duration(24, 4)
for voice_name, timespan_inventory in demultiplexed_maquette.items():
    silent_timespans = timespantools.TimespanInventory([
        consort.SilentTimespan(
            start_offset=0,
            stop_offset=segment_duration,
            voice_name=voice_name,
            ),
        ])
    for timespan in timespan_inventory:
        silent_timespans -= timespan
    timespan_inventory.extend(silent_timespans)
    timespan_inventory.sort()

show(demultiplexed_maquette, range_=(0, 6))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> segment_duration = Duration(24, 4)
>>> for voice_name, timespan_inventory in demultiplexed_maquette.items():
...     silent_timespans = timespantools.TimespanInventory([
...         consort.SilentTimespan(
...             start_offset=0,
...             stop_offset=segment_duration,
...             voice_name=voice_name,
...             ),
...         ])
...     for timespan in timespan_inventory:
...         silent_timespans -= timespan
...     timespan_inventory.extend(silent_timespans)
...     timespan_inventory.sort()
...
>>> show(demultiplexed_maquette, range_=(0, 6))
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-2dd23f0a7928bc8e96fba01888af4593.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent The above assumes a segment duration of 6 instead of the maquette's
initial duration of \sfrac[big]{21}{4}, and therefore pads out the end of each
timespan inventory to that stop offset with silence.

\subsection{Rewriting meters, revisited}
\label{ssec:rewriting-meters-revisited}

Once its maquette is completely populated, Consort's segment-maker performs
meter rewriting. This proceeds in a more elaborate manner than the meter
rewriting process as outlined in \autoref{sec:rewriting-meters} and
\autoref{ssec:rewriting-meters}, and involves a number of notable differences.

For reasons of computational efficiency, Consort rewrites the meters in
each phrase of music annotating each performed timespan in the maquette before
those phrases have even been inserted into the segment-maker's score. Meter
rewriting involves potentially many alterations to the contents of containers
due to fusing and splitting, as well as many duration and offset lookups.
Anytime a component is replaced or its duration changed, the cached offsets of
components located in the score tree after the changed component as well as
the durations of its parents are invalidated. They must be recomputed and
re-cached on the next offset lookup performed on any component the score tree.
Delaying inserting each inscribed timespan's music into the segment-maker's
score guarantees that that music's score depth remains shallow, and therefore
limits the complexity of offset calculation during rewriting.

When beginning the meter rewriting process, the segment-maker converts its
fitted meters into a timespan collection -- Consort's optimized timespan
inventory class -- containing timespans annotated with those fitted meters, one
per timespan.

\begin{comment}
<abjad>
meters = metertools.MeterInventory([(3, 4), (2, 4), (6, 8), (5, 16)])
meter_timespans = consort.SegmentMaker.meters_to_timespans(meters)
print(format(meter_timespans))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> meters = metertools.MeterInventory([(3, 4), (2, 4), (6, 8), (5, 16)])
>>> meter_timespans = consort.SegmentMaker.meters_to_timespans(meters)
>>> print(format(meter_timespans))
consort.tools.TimespanCollection(
    [
        timespantools.AnnotatedTimespan(
            start_offset=durationtools.Offset(0, 1),
            stop_offset=durationtools.Offset(3, 4),
            annotation=metertools.Meter(
                '(3/4 (1/4 1/4 1/4))'
                ),
            ),
        timespantools.AnnotatedTimespan(
            start_offset=durationtools.Offset(3, 4),
            stop_offset=durationtools.Offset(5, 4),
            annotation=metertools.Meter(
                '(2/4 (1/4 1/4))'
                ),
            ),
        timespantools.AnnotatedTimespan(
            start_offset=durationtools.Offset(5, 4),
            stop_offset=durationtools.Offset(2, 1),
            annotation=metertools.Meter(
                '(6/8 ((3/8 (1/8 1/8 1/8)) (3/8 (1/8 1/8 1/8))))'
                ),
            ),
        timespantools.AnnotatedTimespan(
            start_offset=durationtools.Offset(2, 1),
            stop_offset=durationtools.Offset(37, 16),
            annotation=metertools.Meter(
                '(5/16 ((3/16 (1/16 1/16 1/16)) (2/16 (1/16 1/16))))'
                ),
            ),
        ]
    )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Representing meters as timespans provides two important benefits.
First, meters intersecting a given division within a phrase can be efficiently
located using the search methods implemented on \texttt{TimespanCollection}.
Second, the time relation methods implemented on \texttt{Timespan} can be used
to test if a given division's timespan is congruent -- that is, possesses an
identical start and stop offset -- to a meter's timespan. Divisions containing
solely rests which are also congruent to a meter timespan can be rewritten with
full-bar rests.

The segment-maker then proceeds through each demultiplexed timespan inventory
in the maquette, iterating over each timespan, then over each division in that
performed timespan's music. Timespans whose rhythm-maker forbids meter
rewriting -- via the \texttt{forbid\_meter\_rewriting} flag on an optional
\texttt{Duration\-Spelling\-Specifier} -- are skipped over.\footnote{It may be
undesirable to rewrite a rhythm's meter in certain situations, particularly if
a composer is trying to avoid the introduction of ties or dots for whatever
reason.} In order to determine which meter governs a division, that division's
timespan must be retrieved and then translated before it can be used to search
the inventory of meter timespans. Because each phrase of music annotating each
performed timespan has not yet been inserted into the score, they all express
their start offset as 0. Likewise, each phrase's child divisions express their
start offsets relative to their parent phrase's 0 start offset. Translating
each division's timespan relative to the start offset of the performed timespan
annotated by that division's phrase provides a useful search target for the
meter timespan inventory. The translated division timespan represents the
timespan that division \emph{would} occupy if its phrase, and all other
phrases, had already been inserted into the appropriate voice in the score.
Intersecting meters can then be found through a simple search and retranslated
relative to the current performed timespan's start offset, giving their
appropriate location within the not-yet-inserted phrase. The following
operations outline the translation and search process:

\begin{comment}
<abjad>
inscribed_timespan = consort.PerformedTimespan(
    start_offset=(5, 4),
    stop_offset=(9, 5),
    music=Container("{ c'4 }{ c'2 }{ c'4 }"),
    )
division = inscribed_timespan.music[1]
division_timespan = inspect_(division).get_timespan()
print(format(division_timespan))
translation = inscribed_timespan.start_offset
division_timespan = division_timespan.translate(translation)
print(format(division_timespan))
meter_timespan = meter_timespans.find_timespans_intersecting_timespan(
    division_timespan)[0]
print(format(meter_timespan))
translation = -1 * division_timespan.start_offset
meter_timespan = meter_timespan.translate(translation)
print(format(meter_timespan))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> inscribed_timespan = consort.PerformedTimespan(
...     start_offset=(5, 4),
...     stop_offset=(9, 5),
...     music=Container("{ c'4 }{ c'2 }{ c'4 }"),
...     )
>>> division = inscribed_timespan.music[1]
>>> division_timespan = inspect_(division).get_timespan()
>>> print(format(division_timespan))
timespantools.Timespan(
    start_offset=durationtools.Offset(1, 4),
    stop_offset=durationtools.Offset(3, 4),
    )
\end{minted}
\begin{minted}{pycon}
>>> translation = inscribed_timespan.start_offset
>>> division_timespan = division_timespan.translate(translation)
>>> print(format(division_timespan))
timespantools.Timespan(
    start_offset=durationtools.Offset(3, 2),
    stop_offset=durationtools.Offset(2, 1),
    )
\end{minted}
\begin{minted}{pycon}
>>> meter_timespan = meter_timespans.find_timespans_intersecting_timespan(
...     division_timespan)[0]
>>> print(format(meter_timespan))
timespantools.AnnotatedTimespan(
    start_offset=durationtools.Offset(5, 4),
    stop_offset=durationtools.Offset(2, 1),
    annotation=metertools.Meter(
        '(6/8 ((3/8 (1/8 1/8 1/8)) (3/8 (1/8 1/8 1/8))))'
        ),
    )
\end{minted}
\begin{minted}{pycon}
>>> translation = -1 * division_timespan.start_offset
>>> meter_timespan = meter_timespan.translate(translation)
>>> print(format(meter_timespan))
timespantools.AnnotatedTimespan(
    start_offset=durationtools.Offset(-1, 4),
    stop_offset=durationtools.Offset(1, 2),
    annotation=metertools.Meter(
        '(6/8 ((3/8 (1/8 1/8 1/8)) (3/8 (1/8 1/8 1/8))))'
        ),
    )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent With the appropriate meters selected, rewriting continues very much
as described in \autoref{ssec:rewriting-meters}. Tuplets are rewritten solely
with respect for the pre-prolated contents durations, and unprolated containers
are rewritten with respect for their intersecting meter, with an initial
offsets applied to the meter rewriting process if they happen to start later
than their meter. Additionally, Consort's meter rewriting tests silent meters
-- those whose leaves consist entirely of rests -- for congruency with the
current meter. Any division consisting solely of rests which also begins and
ends at the start and stop offsets of a meter's timespan can be rewritten
instead as a single full-bar rest. The segment-maker also attaches a
\texttt{StaffLinesSpanner} to the silent division, which collapses the staff
down to a single line, giving the score a fragmented appearance. Finally,
Consort's segment-maker performs a logical-tie cleanup pass, fusing all
2-length logical ties consisting of matched pairs of \sfrac[big]{1}{16} or
\sfrac[big]{1}{32} notes. This takes care of some possible artifacts of heavily
subdivided meter-rewriting, and makes the final rhythmic output generally more
readable.

\subsection{Populating the score}
\label{ssec:populating-the-score}

After meter-rewriting, the segment-maker can finally populate its score. To do
so, it iterates through its timespan maquette and still-unpopulated score in
parallel, extracting the inscribed music from each performed timespan in the
maquette and inserting those phrases into the score in the appropriate voice.
With the segment-maker's score populated, rhythmic interpretation ends and
non-rhythmic interpretation may begin.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Non-rhythmic interpretation}
\label{sec:non-rhythmic-interpretation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Non-rhythmic interpretation involves the process of progressively embellishing
the contents of the rhythmically-interpreted score while preserving the score
hierarchy and attack-point structure. During non-rhythmic interpretation, the
segment-maker may attach grace notes, attach indicators and spanners, change
the pitches of notes or even replace those notes with chords.

\subsection{Score traversal}
\label{ssec:score-traversal}

Recall the earlier discussion in \autoref{ssec:iteration} regarding score
iteration techniques. Of the various possible types of iteration, Consort's
handlers primarily make use of two during non-rhythmic interpretation. The
first -- \emph{attack-point} or \emph{time-wise logical tie} iteration --
iterates through all logical ties in the score in \emph{time order} according
to their start offset in the score, regardless of their vertical position
within the score. Logical ties with identical start offsets -- those appearing
at simultaneities across voices -- are then sorted by their \emph{score order}.
This results in an iteration which moves first forward in time and the
top-of-score to bottom. The second -- \emph{phrase-wise} iteration -- locates
each voice context in the score, then iterates through the top-level containers
in that voice. These top-level containers are the same containers reference by
each performed timespan's \texttt{music} property, and represent each phrase in
the maquette. Attack-point iteration helps maintain the continuity of some
process across multiple voices in time, while phrase-wise iteration returns
entire phrases and therefore allows processes to treat those phrases and all of
the components located in the subtree rooted at the phrase as a group.

Consider the following two voice score, produced by a segment-maker and
annotated to show each division and phrase:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = consort.MusicSpecifier(
    attachment_handler=consort.AttachmentHandler(),
    rhythm_maker=rhythmmakertools.TaleaRhythmMaker(
        extra_counts_per_division=(0, 1),
        talea=rhythmmakertools.Talea([2, 3, 2, 4], 16),
        ),
    )
timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea([0, 1], 4),
    playing_groupings=(1, 2, 2, 1, 2),
    playing_talea=rhythmmakertools.Talea([2, 3], 8),
    silence_talea=rhythmmakertools.Talea([1, 2, 3, 4], 8),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = consort.SegmentMaker(
    desired_duration_in_seconds=8,
    discard_final_silence=True,
    permitted_time_signatures=[(2, 4), (5, 16), (3, 4)],
    score_template=templatetools.GroupedRhythmicStavesScoreTemplate(
        staff_count=2,
        with_clefs=True,
        ),
    settings=[music_setting],
    tempo=indicatortools.Tempo((1, 4), 72),
    )
illustration, metadata = segment_maker(annotate=True, verbose=False)
show(illustration)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = consort.MusicSpecifier(
...     attachment_handler=consort.AttachmentHandler(),
...     rhythm_maker=rhythmmakertools.TaleaRhythmMaker(
...         extra_counts_per_division=(0, 1),
...         talea=rhythmmakertools.Talea([2, 3, 2, 4], 16),
...         ),
...     )
>>> timespan_maker = consort.TaleaTimespanMaker(
...     initial_silence_talea=rhythmmakertools.Talea([0, 1], 4),
...     playing_groupings=(1, 2, 2, 1, 2),
...     playing_talea=rhythmmakertools.Talea([2, 3], 8),
...     silence_talea=rhythmmakertools.Talea([1, 2, 3, 4], 8),
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = consort.SegmentMaker(
...     desired_duration_in_seconds=8,
...     discard_final_silence=True,
...     permitted_time_signatures=[(2, 4), (5, 16), (3, 4)],
...     score_template=templatetools.GroupedRhythmicStavesScoreTemplate(
...         staff_count=2,
...         with_clefs=True,
...         ),
...     settings=[music_setting],
...     tempo=indicatortools.Tempo((1, 4), 72),
...     )
>>> illustration, metadata = segment_maker(annotate=True, verbose=False)
>>> show(illustration)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-b7d7f3610d178121f1b9a46ae374732b.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent At the beginning of non-rhythmic interpretation, the segment-maker
constructs an \emph{attack-point map}: a mapping of pitched logical ties
against \emph{attack-point signatures}. Consort's \texttt{AttackPointSignature}
class caches information about the location of each pitched logical tie in the
segment, including its normalized position within its division, phrase and
segment and its indices within the same. The segment-maker constructs the
attack-point map by iterating over all note and chords in its score in
time-wise order. It selects the logical tie for each iterated leaf, skipping
those for which the leaf is not the logical tie's head, and constructs an
attack-point signature for the logical tie, storing both in an ordered
dictionary. The segment-maker caches its attack-point map on its instance,
allowing it to be referenced by its own methods, as well as examined after
interpretation completes.

\begin{comment}
<abjad>[stylesheet=../consort.ily]
attack_point_map = segment_maker.attack_point_map
all_pairs = list(attack_point_map.items())
first_logical_tie, first_attack_point_signature = all_pairs[0]
print(first_logical_tie)
print(format(first_attack_point_signature))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> attack_point_map = segment_maker.attack_point_map
>>> all_pairs = list(attack_point_map.items())
>>> first_logical_tie, first_attack_point_signature = all_pairs[0]
>>> print(first_logical_tie)
LogicalTie(Note("c'8"),)
\end{minted}
\begin{minted}{pycon}
>>> print(format(first_attack_point_signature))
consort.tools.AttackPointSignature(
    division_index=0,
    division_position=durationtools.Multiplier(0, 1),
    logical_tie_index=0,
    phrase_position=durationtools.Multiplier(0, 1),
    segment_position=durationtools.Multiplier(0, 1),
    total_divisions_in_phrase=1,
    total_logical_ties_in_division=2,
    )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Because the attack-point map was constructed in time-wise order, each
cached logical tie can be visited in time order when iterating over it. The
following example iterates over the attack-point map, retrieving each key/value
pair -- comprised of a logical-tie selection and an attack-point signature --
while enumerating them -- producing the index of that pair, e.g. first, second,
third, etc. -- and attaches some markup to the head of that logical tie
comprised of its index formatted within a padded box:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
for index, key_value_pair in enumerate(attack_point_map.items()):
    logical_tie, attack_point_signature = key_value_pair
    markup = Markup(index, Up)
    markup = markup.smaller().pad_around(0.25).box()
    attach(markup, logical_tie.head)

show(illustration)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> for index, key_value_pair in enumerate(attack_point_map.items()):
...     logical_tie, attack_point_signature = key_value_pair
...     markup = Markup(index, Up)
...     markup = markup.smaller().pad_around(0.25).box()
...     attach(markup, logical_tie.head)
...
>>> show(illustration)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-e5eb14d082b377be235f48d77c41e857.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Attack-point signatures cache each logical-tie's \emph{logical tie
index} and \emph{division index}. A logical tie's logical tie index gives its
index within the list of all logical ties starting within its division. A
logical tie's division index give the index of the division within which it
starts within the phrase itself. The following iteration shows the division
index and logical tie index of each attack-point, boxed and separated by a
colon. The annotation brackets clarify the divisions within each phrase:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
for logical_tie, attack_point_signature in attack_point_map.items():
    for markup in inspect_(logical_tie.head).get_markup():
        detached = detach(markup, logical_tie.head)
    string = '{}:{}'.format(
        attack_point_signature.division_index,
        attack_point_signature.logical_tie_index,
        )
    markup = Markup(string, Up)
    markup = markup.smaller().pad_around(0.25).box()
    attach(markup, logical_tie.head)

show(illustration)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> for logical_tie, attack_point_signature in attack_point_map.items():
...     for markup in inspect_(logical_tie.head).get_markup():
...         detached = detach(markup, logical_tie.head)
...     string = '{}:{}'.format(
...         attack_point_signature.division_index,
...         attack_point_signature.logical_tie_index,
...         )
...     markup = Markup(string, Up)
...     markup = markup.smaller().pad_around(0.25).box()
...     attach(markup, logical_tie.head)
...
>>> show(illustration)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-5048cc902d0aef79e502179a93f969d0.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Attack-point signatures maintain the position of each logical tie's
start offset normalized between 0 and 1 within its division, as well as its
phrase. The following iteration show each logical tie's phrase position as a
fraction, after detaching the previously attached markup:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
for logical_tie, attack_point_signature in attack_point_map.items():
    for markup in inspect_(logical_tie.head).get_markup():
        detached = detach(markup, logical_tie.head)
    phrase_position = attack_point_signature.phrase_position
    markup = Markup.fraction(phrase_position)
    markup = Markup(markup, Up)
    markup = markup.smaller().pad_around(0.25).box()
    attach(markup, logical_tie.head)

show(illustration)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> for logical_tie, attack_point_signature in attack_point_map.items():
...     for markup in inspect_(logical_tie.head).get_markup():
...         detached = detach(markup, logical_tie.head)
...     phrase_position = attack_point_signature.phrase_position
...     markup = Markup.fraction(phrase_position)
...     markup = Markup(markup, Up)
...     markup = markup.smaller().pad_around(0.25).box()
...     attach(markup, logical_tie.head)
...
>>> show(illustration)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-c70c0411da81cbcb66ede4abb54d0f06.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Attack-point signatures also maintain the position of each logical
tie's start offset in the context of the entire segment's timespan:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
for logical_tie, attack_point_signature in attack_point_map.items():
    for markup in inspect_(logical_tie.head).get_markup():
        detached = detach(markup, logical_tie.head)
    segment_position = attack_point_signature.segment_position
    markup = Markup.fraction(segment_position)
    markup = Markup(markup, Up)
    markup = markup.smaller().pad_around(0.25).box()
    attach(markup, logical_tie.head)

show(illustration)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> for logical_tie, attack_point_signature in attack_point_map.items():
...     for markup in inspect_(logical_tie.head).get_markup():
...         detached = detach(markup, logical_tie.head)
...     segment_position = attack_point_signature.segment_position
...     markup = Markup.fraction(segment_position)
...     markup = Markup(markup, Up)
...     markup = markup.smaller().pad_around(0.25).box()
...     attach(markup, logical_tie.head)
...
>>> show(illustration)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-1fac8cdbf07f60d92bbfadaf293f540e.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Consort's phrase-wise iteration can be demonstrated by building an
iterator function. The following function iterates through its \texttt{score}
argument by voice, then iterates over the top-level containers within that
voice. For each iterated container, it checks if that container's
\emph{effective} music specifier is equivalent to the \enquote{silent} music
specifier which Consort attaches to its silent phrases. If the music specifiers
match, the phrase must be silent, and the function \emph{continues} past it. If
the music specifiers don't match, the phrase must have some significant musical
content, and therefore the function yields that phrase.

\begin{comment}
<abjad>[stylesheet=../consort.ily]
def iterate_phrasewise(score):
    prototype = consort.MusicSpecifier
    silent_music_specifier = consort.MusicSpecifier()
    for voice in iterate(score).by_class(Voice):
        for phrase in voice:
            music_specifier = inspect_(phrase).get_effective(prototype)
            if music_specifier == silent_music_specifier:
                continue
            yield phrase
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> def iterate_phrasewise(score):
...     prototype = consort.MusicSpecifier
...     silent_music_specifier = consort.MusicSpecifier()
...     for voice in iterate(score).by_class(Voice):
...         for phrase in voice:
...             music_specifier = inspect_(phrase).get_effective(prototype)
...             if music_specifier == silent_music_specifier:
...                 continue
...             yield phrase
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Before running the iterator to attach the phrase-index markup, the
previously attached markup must be removed:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
for logical_tie in attack_point_map.keys():
    for markup in inspect_(logical_tie.head).get_markup():
        detached = detach(markup, logical_tie.head)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> for logical_tie in attack_point_map.keys():
...     for markup in inspect_(logical_tie.head).get_markup():
...         detached = detach(markup, logical_tie.head)
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent By chaining a call to the above-created
\texttt{iterate\_phrasewise()} function with Python's built-in
\texttt{enumerate()}, phrases can be extracted from the score along with their
index. The result simply counts each non-silent phrase left-to-right in each
voice, from the top of the score to the bottom:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
for index, phrase in enumerate(iterate_phrasewise(segment_maker.score)):
    first_leaf = phrase.select_leaves()[0]
    markup = Markup(index, Up)
    markup = markup.smaller().pad_around(0.25).box()
    attach(markup, first_leaf)

show(illustration)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> for index, phrase in enumerate(iterate_phrasewise(segment_maker.score)):
...     first_leaf = phrase.select_leaves()[0]
...     markup = Markup(index, Up)
...     markup = markup.smaller().pad_around(0.25).box()
...     attach(markup, first_leaf)
...
>>> show(illustration)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-a7b8a136d11a9c41272d15952443df11.pdf}
\end{singlespacing}
\end{abjadbookoutput}

Some of the handlers used during non-rhythmic interpretation rely on
information about each pitched logical tie in the score. Both grace- and
pitch-handlers iterate through the score in time order. While grace-handlers do
not make use of information stored within each logical tie's associated
attack-point signature -- in the current implementation of Consort only
pitch-handlers rely on attack-point signatures --, simply caching the
time-order of all logical ties within the segment saves multiple complex
iteration procedures.

\subsection{Grace-handlers}
\label{ssec:grace-handlers}

Consort's segment-maker can be instructed to attach grace notes to various
parts of the score by defining a \emph{grace-handler} on a music-specifier.
Grace-handlers attach *grace containers* to logical ties within a score in a
patterned way, according to a cyclic sequence of counts, as well as collection
of flags which restrict what leaves may be selected for attachment.
Abjad implements grace notes as normal leaves -- notes, chords and rests --
within special components which act both as Abjad \texttt{Container} classes as
well as attachable indicators. That is to say, grace notes must be placed
within one of these grace containers which is then attached to another leaf in
the score much like any other indicator such as a dynamic indication or
articulation. Grace-handlers traverse the score by logical tie in time-order,
iterating over the previously cached ordered dictionary of attack-points which
was generated at the beginning of non-rhythmic interpretation. Consider the
following music specifier, whose grace-handler places grace notes with a cyclic
count of 1, 2, 0, 0, 0.

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = consort.MusicSpecifier(
    grace_handler=consort.GraceHandler(
        counts=(1, 2, 0, 0, 0),
        ),
    rhythm_maker=rhythmmakertools.TaleaRhythmMaker(
        extra_counts_per_division=(0, 1),
        talea=rhythmmakertools.Talea([1, 2, 3, 1, 4], 16),
        ),
    )
timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea([0, 1], 4),
    playing_groupings=(1, 2, 2),
    playing_talea=rhythmmakertools.Talea([2, 3], 8),
    silence_talea=rhythmmakertools.Talea([1, 2, 3, 4], 8),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = consort.SegmentMaker(
    desired_duration_in_seconds=8,
    discard_final_silence=True,
    permitted_time_signatures=[(2, 4), (5, 16), (3, 4)],
    score_template=templatetools.GroupedRhythmicStavesScoreTemplate(
        staff_count=2,
        with_clefs=True,
        ),
    settings=[music_setting],
    tempo=indicatortools.Tempo((1, 4), 72),
    )
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = consort.MusicSpecifier(
...     grace_handler=consort.GraceHandler(
...         counts=(1, 2, 0, 0, 0),
...         ),
...     rhythm_maker=rhythmmakertools.TaleaRhythmMaker(
...         extra_counts_per_division=(0, 1),
...         talea=rhythmmakertools.Talea([1, 2, 3, 1, 4], 16),
...         ),
...     )
>>> timespan_maker = consort.TaleaTimespanMaker(
...     initial_silence_talea=rhythmmakertools.Talea([0, 1], 4),
...     playing_groupings=(1, 2, 2),
...     playing_talea=rhythmmakertools.Talea([2, 3], 8),
...     silence_talea=rhythmmakertools.Talea([1, 2, 3, 4], 8),
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = consort.SegmentMaker(
...     desired_duration_in_seconds=8,
...     discard_final_silence=True,
...     permitted_time_signatures=[(2, 4), (5, 16), (3, 4)],
...     score_template=templatetools.GroupedRhythmicStavesScoreTemplate(
...         staff_count=2,
...         with_clefs=True,
...         ),
...     settings=[music_setting],
...     tempo=indicatortools.Tempo((1, 4), 72),
...     )
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-ad59b3160612abf06340ee99d57e6837.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Note above that the first note in the score by time-wise ordering --
the initial \sfrac[big]{1}{16} note in the upper staff -- does not receive the
expected single grace note. The grace note pattern starts instead on the
\sfrac[big]{1}{8} note that follows. For purely practical reasons, Consort's
grace-handler will \emph{not} put grace notes before the first leaf in any
voice in a score. LilyPond's grace spacing algorithm does not operate well with
its strict proportional spacing algorithm. When spaced strictly proportionally,
grace notes which are positioned before the beat -- via LilyPond's
\texttt{\textbackslash{}grace}, \texttt{\textbackslash{}appoggiatura} or
\texttt{\textbackslash{}acciaccatura} commands -- may end up colliding with
other glyphs in the staff. Consort works around this by using LilyPond's
\texttt{\textbackslash{}afterGrace} command, which places grace notes after the
note to which they attach. This positioning allows grace notes to avoid most
collisions. Unfortunately, this also means that grace notes must always appear
after a note or rest, making it impossible to start a segment with grace notes.

\subsection{Pitch-handlers}
\label{ssec:pitch-handlers}

Consort's \emph{pitch-handlers} apply pitches to logical ties within a score in
a patterned way, through the use of various cyclic sequences applied to each
logical tie in time-wise order, much like grace-handlers. Consider the
following short segment, containing two voices of \sfrac[big]{1}{16} note rhythms
interspersed with rests. Without any pitch-handler specified, both voices
default to creating notes pitched at middle-C:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
segment_maker = consort.SegmentMaker(
    desired_duration_in_seconds=9,
    #omit_stylesheets=True,
    permitted_time_signatures=[(3, 4)],
    score_template=templatetools.GroupedStavesScoreTemplate(
        staff_count=2,
        ),
    tempo=indicatortools.Tempo((1, 4), 60),
    )
music_specifier = consort.MusicSpecifier(
    rhythm_maker=rhythmmakertools.TaleaRhythmMaker(
        talea=rhythmmakertools.Talea([1], 16),
        ),
    )
timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea([0, 1], 4),
    playing_talea=rhythmmakertools.Talea([1], 8),
    playing_groupings=[3],
    silence_talea=rhythmmakertools.Talea([1], 8),
    )
segment_maker.add_setting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> segment_maker = consort.SegmentMaker(
...     desired_duration_in_seconds=9,
...     #omit_stylesheets=True,
...     permitted_time_signatures=[(3, 4)],
...     score_template=templatetools.GroupedStavesScoreTemplate(
...         staff_count=2,
...         ),
...     tempo=indicatortools.Tempo((1, 4), 60),
...     )
>>> music_specifier = consort.MusicSpecifier(
...     rhythm_maker=rhythmmakertools.TaleaRhythmMaker(
...         talea=rhythmmakertools.Talea([1], 16),
...         ),
...     )
>>> timespan_maker = consort.TaleaTimespanMaker(
...     initial_silence_talea=rhythmmakertools.Talea([0, 1], 4),
...     playing_talea=rhythmmakertools.Talea([1], 8),
...     playing_groupings=[3],
...     silence_talea=rhythmmakertools.Talea([1], 8),
...     )
>>> segment_maker.add_setting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-f57c7a2d04e113f3b6a9996faf38c8ec.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent By reconfiguring the above music specifier with an
\texttt{AbsolutePitchHandler} -- a concrete subclass of Consort's abstract
\texttt{PitchHandler} -- different pitches can be applied to each note in the
segment. The following pitch-handler paints a C-major scale across all of the
logical ties in the resulting segment. Note how the G4, A4, B4 and C5 of the
applied C major scale alternate between the voices according to both their
time-wise position and their score order, as discussed in
\autoref{ssec:score-traversal}:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler=consort.AbsolutePitchHandler(
        pitch_specifier="c' d' e' f' g' a' b' c''",
        ),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = new(
...     music_specifier,
...     pitch_handler=consort.AbsolutePitchHandler(
...         pitch_specifier="c' d' e' f' g' a' b' c''",
...         ),
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-db0f70ae167ce8ea9bf04cc4e08fc8fb.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Pitch-handlers apply their pitch patterns in time-wise order
regardless of rhythmic texture:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    rhythm_maker__talea__counts=[1, 2, 3, 4],
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = new(
...     music_specifier,
...     rhythm_maker__talea__counts=[1, 2, 3, 4],
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-255b940024721c19a014512f57aa352c.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent As discussed in \autoref{ssec:inscription}, pitch-handlers, like
rhythm-makers, maintain a seed value keyed to the music specifier in which they
are defined, allowing them to maintain their pattern even in fragmentary or
interrupted musical textures. Note how, when interrupted by phrases defined by
a different music specifier, the previously defined texture continues its pitch
application pattern after each interruption or silence exactly where it left
off. For example, the A4 \sfrac[big]{1}{16} note in the lower voice in measure one
is continued by the B4 \sfrac[big]{1}{8} note in the upper voice at the beginning of
measure two, and followed by the C5 \sfrac[big]{1}{8} note directly below in the
lower voice. The C-major pattern continues despite the intrusion of the triplet
texture defined by the \texttt{other\_music\_specifier}:

%\todo[inline]{\textbf{TODO:} Fix seed-handling at pitch-choice boundaries.}

\begin{comment}
<abjad>[stylesheet=../consort.ily]
other_music_specifier = consort.MusicSpecifier(
    pitch_handler=consort.AbsolutePitchHandler(pitch_specifier='g fs e f'),
    rhythm_maker=rhythmmakertools.EvenDivisionRhythmMaker(
        denominators=[8],
        extra_counts_per_division=(1,),
        ),
    )
other_music_setting = consort.MusicSetting(
    timespan_maker=consort.TaleaTimespanMaker(
        initial_silence_talea=rhythmmakertools.Talea([1], 2),
        silence_talea=rhythmmakertools.Talea([1], 2),
        ),
    v1=other_music_specifier,
    v2=other_music_specifier,
    )
segment_maker = new(
    segment_maker,
    settings=[music_setting, other_music_setting],
    )
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> other_music_specifier = consort.MusicSpecifier(
...     pitch_handler=consort.AbsolutePitchHandler(pitch_specifier='g fs e f'),
...     rhythm_maker=rhythmmakertools.EvenDivisionRhythmMaker(
...         denominators=[8],
...         extra_counts_per_division=(1,),
...         ),
...     )
>>> other_music_setting = consort.MusicSetting(
...     timespan_maker=consort.TaleaTimespanMaker(
...         initial_silence_talea=rhythmmakertools.Talea([1], 2),
...         silence_talea=rhythmmakertools.Talea([1], 2),
...         ),
...     v1=other_music_specifier,
...     v2=other_music_specifier,
...     )
>>> segment_maker = new(
...     segment_maker,
...     settings=[music_setting, other_music_setting],
...     )
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-cab9e9e570855c1c49cd7ea02fda795e.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Pitch-handlers may define explicit \emph{pitch specifiers}, which
behave somewhat analogously to the ratio-parts expressions discussed in
\autoref{ssec:populating-the-maquette}, describing which pitches or
pitch-classes are to be used in which sections of a segment, as partitioned by
a ratio.

\begin{comment}
<abjad>
pitch_specifier = consort.PitchSpecifier(
    pitch_segments=(
        "c' e' g'",
        "fs' g' a'",
        "b d'",
        ),
    ratio=(1, 2, 3),
    )
pitch_choice_timespans = consort.PitchHandler.get_pitch_choice_timespans(
    pitch_specifier=pitch_specifier,
    duration=segment_maker.desired_duration,
    )
print(format(pitch_choice_timespans))
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> pitch_specifier = consort.PitchSpecifier(
...     pitch_segments=(
...         "c' e' g'",
...         "fs' g' a'",
...         "b d'",
...         ),
...     ratio=(1, 2, 3),
...     )
>>> pitch_choice_timespans = consort.PitchHandler.get_pitch_choice_timespans(
...     pitch_specifier=pitch_specifier,
...     duration=segment_maker.desired_duration,
...     )
>>> print(format(pitch_choice_timespans))
consort.tools.TimespanCollection(
    [
        timespantools.AnnotatedTimespan(
            start_offset=durationtools.Offset(0, 1),
            stop_offset=durationtools.Offset(3, 8),
            annotation=datastructuretools.CyclicTuple(
                [
                    pitchtools.NamedPitch("c'"),
                    pitchtools.NamedPitch("e'"),
                    pitchtools.NamedPitch("g'"),
                    ]
                ),
            ),
        timespantools.AnnotatedTimespan(
            start_offset=durationtools.Offset(3, 8),
            stop_offset=durationtools.Offset(9, 8),
            annotation=datastructuretools.CyclicTuple(
                [
                    pitchtools.NamedPitch("fs'"),
                    pitchtools.NamedPitch("g'"),
                    pitchtools.NamedPitch("a'"),
                    ]
                ),
            ),
        timespantools.AnnotatedTimespan(
            start_offset=durationtools.Offset(9, 8),
            stop_offset=durationtools.Offset(9, 4),
            annotation=datastructuretools.CyclicTuple(
                [
                    pitchtools.NamedPitch('b'),
                    pitchtools.NamedPitch("d'"),
                    ]
                ),
            ),
        ]
    )
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\noindent When used to reconfigure the previously-defined music specifier's
pitch-handler, each of the pitch specifier's pitch segments appears in only one
portion of the resulting score, as defined by the pitch-choice timespan
inventory:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__pitch_specifier=pitch_specifier,
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = new(
...     music_specifier,
...     pitch_handler__pitch_specifier=pitch_specifier,
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-568ee280070738d0c0f4fe1cfa58542f.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Likewise, a pitch-handler may define a pitch-operation specifier,
which behaves similarly to pitch specifiers. Pitch-operation specifiers combine
a sequence of pitch-operations -- transposition, inversion, retrogression,
rotation, etc. -- with a ratio defining where those operations should appear
during the course of a segment. Pitch-operation specifiers may be used
alongside pitch specifiers, and with differing ratios. The following
pitch-operation specifier partitions the segment into three unequal parts,
applying an operation to the first \sfrac[big]{1}{4}, no operation to the middle
\sfrac[big]{1}{2}, and another operation to the final \sfrac[big]{1}{4} of the segment:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__pitch_operation_specifier=consort.PitchOperationSpecifier(
        pitch_operations=(
            pitchtools.PitchOperation((
                pitchtools.Inversion(),
                )),
            None,
            pitchtools.PitchOperation((
                pitchtools.Rotation(-1),
                pitchtools.Transposition(-1),
                ))
            ),
        ratio=(1, 2, 1),
        ),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = new(
...     music_specifier,
...     pitch_handler__pitch_operation_specifier=consort.PitchOperationSpecifier(
...         pitch_operations=(
...             pitchtools.PitchOperation((
...                 pitchtools.Inversion(),
...                 )),
...             None,
...             pitchtools.PitchOperation((
...                 pitchtools.Rotation(-1),
...                 pitchtools.Transposition(-1),
...                 ))
...             ),
...         ratio=(1, 2, 1),
...         ),
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-11b3c478808b10829f3f29873c8ba94a.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Pitch-handlers may also be configured to forbid immediate pitch
repetitions within the same voice via the \texttt{forbid\_repetitions} flag,
assuming the current logical tie has more than one pitch available via its
pitch-choices:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__forbid_repetitions=True,
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = new(
...     music_specifier,
...     pitch_handler__forbid_repetitions=True,
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-67ad79bdb943d1a630d029e123f47aa1.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Chords may be applied in a patterned way to the logical ties a
pitch-handler iterates over by defining a sequence of \emph{logical tie
expressions} -- classes which, when called on a logical tie, modify or replace
each of the leaves in that logical tie, leaving the tie itself in place.
Consort provides a few subclasses of logical tie expression, such as the
\texttt{ChordExpression}, \texttt{HarmonicExpression} and
\texttt{KeyClusterExpression}. When called on a logical tie, chord expression
replace each note in the tie with a chord whose pitches may be specified either
absolutely or in terms of intervals relative to the original pitch. Note that
Python's \texttt{None}, like in the previously defined pitch-operation
specifier, acts as a \enquote{no-op}, making no change at all:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__logical_tie_expressions=(
        consort.ChordExpression(chord_expr=(-2, 0, 2)),
        consort.ChordExpression(chord_expr=(-7, 0, 7)),
        None,
        ),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = new(
...     music_specifier,
...     pitch_handler__logical_tie_expressions=(
...         consort.ChordExpression(chord_expr=(-2, 0, 2)),
...         consort.ChordExpression(chord_expr=(-7, 0, 7)),
...         None,
...         ),
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-df60aaa0992062896d0984b4ac7a0d5f.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent All of the above pattern definitions -- pitch specifiers,
pitch-operation specifiers and sequence of logical tie expressions -- continue
to apply in order even over an interrupted or otherwise fragmentary texture:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
segment_maker = new(
    segment_maker,
    settings=[music_setting, other_music_setting],
    )
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> segment_maker = new(
...     segment_maker,
...     settings=[music_setting, other_music_setting],
...     )
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-8a51fe815fa07c9afc93929d1705e318.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Now, consider again the \sfrac[big]{1}{16} note music specifier from
earlier in this section, whose pitch-handler cycled through a C-major scale.
Pitch-handlers may be configure to use the same pitch for each division, or
even phrase, rather than choosing a new pitch for each encountered logical tie.
This behavior is defined by the pitch-handler's \emph{application rate}. The
following pitch handler applies the same pitch from its pitch specifier to each
logical tie within each division. Both the divisions and phrases have been
annotated for clarity:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = consort.MusicSpecifier(
    pitch_handler=consort.AbsolutePitchHandler(
        pitch_application_rate='division',
        pitch_specifier="c' d' e' f' g' a' b' c''",
        ),
    rhythm_maker=rhythmmakertools.EvenDivisionRhythmMaker(denominators=[16]),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
lilypond_file, metadata = segment_maker(verbose=False)
consort.annotate(lilypond_file.score)
show(lilypond_file)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = consort.MusicSpecifier(
...     pitch_handler=consort.AbsolutePitchHandler(
...         pitch_application_rate='division',
...         pitch_specifier="c' d' e' f' g' a' b' c''",
...         ),
...     rhythm_maker=rhythmmakertools.EvenDivisionRhythmMaker(denominators=[16]),
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> lilypond_file, metadata = segment_maker(verbose=False)
>>> consort.annotate(lilypond_file.score)
>>> show(lilypond_file)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-d44fb9737027c78c0effbebbc325885a.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Per-division and per-phrase pitch application rates rely on the
attack-point signature objects collected at the beginning of non-rhythmic
interpretation to determine if any encountered logical tie is the first of its
division or phrase. Pitch-handlers also require more complex seed-tracking
logic -- maintained by a dedicated \texttt{SeedSession} class --, as each
encountered logical tie may not require choosing a new pitch, but could still
require choosing a new logical tie expression. The above music specifier can be
re-configured to apply the same pitch to every logical tie in a phrase:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__pitch_application_rate='phrase',
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
lilypond_file, metadata = segment_maker(annotate=True, verbose=False)
show(lilypond_file)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = new(
...     music_specifier,
...     pitch_handler__pitch_application_rate='phrase',
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> lilypond_file, metadata = segment_maker(annotate=True, verbose=False)
>>> show(lilypond_file)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-a330479520dcb927d7d76f2247e4e982.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Additionally, a pattern of interval deviations may be applied on top
of any chosen pitches, regardless of the pitch-handler's application rate. This
provides a mechanism for intermittently varying an otherwise stable melodic
contour:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    pitch_handler__deviations=(0, 0, '-m2', '+m2'),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
lilypond_file, metadata = segment_maker(annotate=True, verbose=False)
show(lilypond_file)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = new(
...     music_specifier,
...     pitch_handler__deviations=(0, 0, '-m2', '+m2'),
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> lilypond_file, metadata = segment_maker(annotate=True, verbose=False)
>>> show(lilypond_file)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-0e934e72a0aded5b4d9d8f7c37c6f6fb.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Other pitch-handlers are possible. For example, Consort's
\texttt{PitchClassPitchHandler} first applies pitch-classes to each logical
tie, then an octavation according a \emph{register specifier} which acts
similarly to a break-point function, shifting the pitch-class up and down by
octaves according to the shape of the specifier and the logical tie's position
within its division, phrase and segment. Pitch-handlers based on vertical
sonoroties, spectral information, or various other techniques could also be
implemented. As it stands, I have so far only implemeted two for the pieces I
have written with Consort, finding them sufficient for now.

\subsection{Attachment-handlers}
\label{ssec:attachment-handlers}

Attachment-handlers manage the process of attaching indicators and spanners to
selections of components within a score. Unlike grace-handlers and
pitch-handlers, they iterate over the score by voice and phrase, rather than by
logical-tie time-wise. Phrase-wise iteration allows attachment-handlers to
create highly-contextualized attachments, considering each component in a
phrase in terms of each other component. Attachment-handlers aggregate
\emph{attachment expressions}, objects pairing a \emph{component selector} --
as described in \autoref{ssec:selectors} -- and an iterable of attachments --
indicators and spanners -- or component expressions. Selectors chain together a
series of \emph{callbacks} which progressively refine a selection as each
callback processes it. Much like the other handler classes described already,
an attachment-expression's attachments sequence cycles, allowing different
groups of attachments to be attached each time the attachment-expression is
called with a different seed value. Attachment-handler associate their
attachment expressions with underscore-delimited string keys. This mechanism,
nearly identical to that employed by music settings for associating music
specifiers with voice-name abbreviations, allows attachment expressions --
which may have an arbitrary number of such associations -- to be reconfigured
through templating to add new attachment expressions or overwrite or nullify
specific existing expressions.\footnote{The principle employed by both music
settings and attachment handlers is crucial: named references beat positional
references.}

Consider the following two-staff segment containing phrases interspersed by
rests and a surface-rhythm with a variety of tuplets and ties:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = consort.MusicSpecifier(
    attachment_handler=consort.AttachmentHandler(),
    rhythm_maker=rhythmmakertools.TaleaRhythmMaker(
        extra_counts_per_division=(0, 1),
        talea=rhythmmakertools.Talea([1, 2, 3, 1, 4], 16),
        ),
    )
timespan_maker = consort.TaleaTimespanMaker(
    initial_silence_talea=rhythmmakertools.Talea([0, 1], 4),
    playing_groupings=(1, 2, 2),
    playing_talea=rhythmmakertools.Talea([2, 3], 8),
    silence_talea=rhythmmakertools.Talea([1, 2, 3, 4], 8),
    )
music_setting = consort.MusicSetting(
    timespan_maker=timespan_maker,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = consort.SegmentMaker(
    desired_duration_in_seconds=8,
    discard_final_silence=True,
    permitted_time_signatures=[(2, 4), (5, 16), (3, 4)],
    score_template=templatetools.GroupedRhythmicStavesScoreTemplate(
        staff_count=2,
        with_clefs=True,
        ),
    settings=[music_setting],
    tempo=indicatortools.Tempo((1, 4), 72),
    )
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = consort.MusicSpecifier(
...     attachment_handler=consort.AttachmentHandler(),
...     rhythm_maker=rhythmmakertools.TaleaRhythmMaker(
...         extra_counts_per_division=(0, 1),
...         talea=rhythmmakertools.Talea([1, 2, 3, 1, 4], 16),
...         ),
...     )
>>> timespan_maker = consort.TaleaTimespanMaker(
...     initial_silence_talea=rhythmmakertools.Talea([0, 1], 4),
...     playing_groupings=(1, 2, 2),
...     playing_talea=rhythmmakertools.Talea([2, 3], 8),
...     silence_talea=rhythmmakertools.Talea([1, 2, 3, 4], 8),
...     )
>>> music_setting = consort.MusicSetting(
...     timespan_maker=timespan_maker,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = consort.SegmentMaker(
...     desired_duration_in_seconds=8,
...     discard_final_silence=True,
...     permitted_time_signatures=[(2, 4), (5, 16), (3, 4)],
...     score_template=templatetools.GroupedRhythmicStavesScoreTemplate(
...         staff_count=2,
...         with_clefs=True,
...         ),
...     settings=[music_setting],
...     tempo=indicatortools.Tempo((1, 4), 72),
...     )
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-0aa18de5de49e47075994670124b817b.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Accents can be attached to the first leaf of each phrase by
configuring the music specifier's attachment-handler with an attachment
expression whose attachments are simply an instance of an Abjad
\texttt{Articulation} and whose selector has been configured to first select
all leaves and then select the first item in that selection of leaves: the
first leaf:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
attachment_expression = consort.AttachmentExpression(
    attachments=Articulation('accent'),
    selector=selectortools.Selector().by_leaves()[0],
    )
music_specifier = new(
    music_specifier,
    attachment_handler__accents=attachment_expression,
    )
music_setting = new(
    music_setting,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker,verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> attachment_expression = consort.AttachmentExpression(
...     attachments=Articulation('accent'),
...     selector=selectortools.Selector().by_leaves()[0],
...     )
>>> music_specifier = new(
...     music_specifier,
...     attachment_handler__accents=attachment_expression,
...     )
>>> music_setting = new(
...     music_setting,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> show(segment_maker,verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-74a87a70e79cad83a7b37f0c731a1095.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Tenuto articulations can be attached to the head of every logical tie
in each phrase -- except the very first -- with a more complex selector. The
following tenuto-attaching attachment-expression's selector first selects all
leaves in each phrase, then selects all pitched logical ties, then selects all
but the first of those logical ties -- rather than all but the first leaf in
\emph{each} of those logical ties -- and finally selects the first leaf in each
selected logical tie:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    attachment_handler__tenuti=consort.AttachmentExpression(
        attachments=Articulation('tenuto'),
        selector=selectortools.Selector()
            .by_leaves()
            .by_logical_tie(pitched=True)
            .get_slice(start=1, apply_to_each=False)
            [0]
        ),
    )
music_setting = new(
    music_setting,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = new(
...     music_specifier,
...     attachment_handler__tenuti=consort.AttachmentExpression(
...         attachments=Articulation('tenuto'),
...         selector=selectortools.Selector()
...             .by_leaves()
...             .by_logical_tie(pitched=True)
...             .get_slice(start=1, apply_to_each=False)
...             [0]
...         ),
...     )
>>> music_setting = new(
...     music_setting,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-382f01f75671130f9db854be5d4b58b4.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Attaching a spanner to the entirety of a phrase is a much simpler
process. When configuring an attachment-handler with a new key, if the value of
that key is not already an attachment expression, the attachment-handler
creates a new attachment expression and sets the value as the attachment
expression's attachment list. Likewise, an attachment expression which has no
selector defined uses a default selector when called, which simply selects
whatever the attachment expression was called against. In the following
reconfiguration, a slur is attached to the entirety of each phrase, using the
configuration coercion described above:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    attachment_handler__slurs=Slur()
    )
music_setting = new(
    music_setting,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
show(segment_maker, verbose=False)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = new(
...     music_specifier,
...     attachment_handler__slurs=Slur()
...     )
>>> music_setting = new(
...     music_setting,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> show(segment_maker, verbose=False)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-fe30dc37283bfd57872ca407b9e341ac.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Attachment-handlers and their attachment expressions can also
delegate to objects specifically designed for attaching indicators and
spanners, much like the logical tie expressions outlined in
\autoref{ssec:pitch-handlers}. For example, Consort's \emph{dynamic expression}
attaches dynamic indications, potentially spanned by hairpins, to the first
leaf in each division of a phrase, and to the last leaf of the phrase as well,
taking care of special cases such as divisions with insufficient numbers of
leaves. Dynamic expressions encapsulate their own selection logic which is
generally too difficult to implement simply through chaining component selector
callbacks. Consider the locations of the dynamics in the following annotated
segment:

\begin{comment}
<abjad>[stylesheet=../consort.ily]
music_specifier = new(
    music_specifier,
    attachment_handler__dynamics=consort.DynamicExpression(['f', 'p'])
    )
music_setting = new(
    music_setting,
    v1=music_specifier,
    v2=music_specifier,
    )
segment_maker = new(segment_maker, settings=[music_setting])
lilypond_file, metadata = segment_maker(annotate=True, verbose=False)
show(lilypond_file)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> music_specifier = new(
...     music_specifier,
...     attachment_handler__dynamics=consort.DynamicExpression(['f', 'p'])
...     )
>>> music_setting = new(
...     music_setting,
...     v1=music_specifier,
...     v2=music_specifier,
...     )
>>> segment_maker = new(segment_maker, settings=[music_setting])
>>> lilypond_file, metadata = segment_maker(annotate=True, verbose=False)
>>> show(lilypond_file)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-aa659a0ea8ee2dcccf4a1afe92996691.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Note above how each division in each phrase begins with a dynamic,
and the last division in each phrase -- including those which are the
\emph{only} division -- contain two dynamics, with the exception of those that
only contain a single logical tie.

\subsection{Expressive attachments}
\label{ssec:expressive-attachments}

\noindent Complex attachment-based formatting scenarios can also be constructed
without relying on component expressions such as \texttt{DynamicExpression}.
One technique used extensively in my scores, especially \emph{Invisible Cities
(ii): Armilla}, is to attach various idiomatic indicators as non-formatting
annotations to components throughout each phrase, and then to attach a
specially-designed spanner to the entirety of each phrase which knows how to
inspect the leaves it covers for the previously attached annotations. For
example, Consort's \texttt{StringContactSpanner} can inspect its leaves for
\texttt{StringContactPoint} indicators and then construct markup for each
contact point -- sul ponticello, ordinario, and so forth -- bridged by arrows
when the contact points changed, and parenthesizing the contact point markup
when cautionary. Likewise, the \texttt{BowContactSpanner} can inspect its
leaves for \texttt{BowContactPoint} indicators -- which indicate the current
position along the hair of the bow -- as well as \texttt{BowMotionTechnique}
indicators -- which describe various motion qualities like \emph{jete} and
tremolo -- replacing the note heads of the staff with fractions for the current
bow position and adding up- or down-bow markup as necessary. When combined with
the appropriate typographic overrides, complex graphic notation results:

\begin{comment}
<abjad>
staff = Staff()
staff.extend(r"c'4. c'8 \times 2/3 { c'4 c'4 c'4 }")
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> staff = Staff()
>>> staff.extend(r"c'4. c'8 \times 2/3 { c'4 c'4 c'4 }")
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\begin{comment}
<abjad>
leaves = staff.select_leaves()
attach(indicatortools.BowMotionTechnique('jete'), leaves[0])
attach(indicatortools.BowContactPoint((1, 4)), leaves[0])
attach(indicatortools.BowContactPoint((3, 4)), leaves[1])
attach(indicatortools.BowContactPoint((1, 2)), leaves[2])
attach(indicatortools.BowMotionTechnique('circular'), leaves[3])
attach(indicatortools.BowContactPoint((1, 1)), leaves[3])
attach(indicatortools.BowContactPoint((0, 1)), leaves[4])
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> leaves = staff.select_leaves()
>>> attach(indicatortools.BowMotionTechnique('jete'), leaves[0])
>>> attach(indicatortools.BowContactPoint((1, 4)), leaves[0])
>>> attach(indicatortools.BowContactPoint((3, 4)), leaves[1])
>>> attach(indicatortools.BowContactPoint((1, 2)), leaves[2])
>>> attach(indicatortools.BowMotionTechnique('circular'), leaves[3])
>>> attach(indicatortools.BowContactPoint((1, 1)), leaves[3])
>>> attach(indicatortools.BowContactPoint((0, 1)), leaves[4])
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\begin{comment}
<abjad>
attach(Clef('percussion'), staff)
override(staff).bar_line.transparent = True
override(staff).dots.staff_position = -8
override(staff).flag.Y_offset = -8.5
override(staff).glissando.bound_details__left__padding = 1.5
override(staff).glissando.bound_details__right__padding = 1.5
override(staff).glissando.thickness = 2
override(staff).script.staff_padding = 3
override(staff).staff_symbol.transparent = True
override(staff).stem.direction = Down
override(staff).stem.length = 8
override(staff).stem.stem_begin_position = -9
override(staff).time_signature.stencil = False
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> attach(Clef('percussion'), staff)
>>> override(staff).bar_line.transparent = True
>>> override(staff).dots.staff_position = -8
>>> override(staff).flag.Y_offset = -8.5
>>> override(staff).glissando.bound_details__left__padding = 1.5
>>> override(staff).glissando.bound_details__right__padding = 1.5
>>> override(staff).glissando.thickness = 2
>>> override(staff).script.staff_padding = 3
>>> override(staff).staff_symbol.transparent = True
>>> override(staff).stem.direction = Down
>>> override(staff).stem.length = 8
>>> override(staff).stem.stem_begin_position = -9
>>> override(staff).time_signature.stencil = False
\end{minted}
\end{singlespacing}
\end{abjadbookoutput}

\begin{comment}
<abjad>
attach(spannertools.BowContactSpanner(), leaves)
show(staff)
</abjad>
\end{comment}

\begin{abjadbookoutput}
\begin{singlespacing}
\vspace{-0.5\baselineskip}
\begin{minted}{pycon}
>>> attach(spannertools.BowContactSpanner(), leaves)
>>> show(staff)
\end{minted}
\noindent\includegraphics[max width=\textwidth,]{assets/lilypond-89bb634d4272124c25c324e04c897de7.pdf}
\end{singlespacing}
\end{abjadbookoutput}

\noindent Notation created in this way treats spanners as typographic
post-processors. The positioning of the various bowing indicators -- the bow
motion technique indicators, the bow contact point indicators -- constitutes
the compositional act, and the bookkeeping inherent to drawing the correctly
formatted lines between them simply constitutes one part of the automated
typesetting process.

\subsection{Post-processing}
\label{ssec:post-processing}

With grace-, pitch- and attachment-handling completed, non-rhythmic
interpretation proceeds to its final step -- post-processing --, during which
various typographic adjustments are made to the score.

First, Consort's segment-maker creates a \emph{floating time-signature context}
-- described in \autoref{ssec:score-post-processing} -- and inserts it as the
first child of the score, assuming the score template did not already provide
for one. The segment-maker then populates its time-signature context with
measures containing typographic spacer-skips, creating the appearance of time
signature indications floating above each bar-line at the top of the score. The
segment-maker attaches other objects to this context as well, such as the its
tempo indication and any configured rehearsal mark or segment name, e.g.
\enquote{Interlude...}. Segments can also be configured with repeat signs or
final bar-line indications. Consort's segment-maker attaches such indicators to
the appropriate leaves in the score during post-processing. Likewise, if a
segment has been configured as the final segment, any colophon information --
the locations and dates of composition -- will be added beneath the last
measure of the score.

More complex post-processing may also be carried out. For reasons related
solely to how LilyPond handles various typographic constructs during
typesetting, it may prove necessary to copy certain voices in the score,
maintaining all rhythmic information therein, but changing their context name
and filtering out various spanners and indicators so that only a specific
subset of typographic commands remain. In doing so, it is possible to isolate
typographic effects to each voice performed \enquote{simultaneously} in a
staff. For example, my score \emph{Invisible Cities (ii): Armilla} employs this
voice-copying technique to create its bowing-staff typography. Up until the
post-processing step of non-rhythmic interpretation, \emph{Armilla}'s bowing
voices contain a rhythm with many indicators and spanners attached -- bow
contact points, string contact points, bow motion techniques, bow contact
spanners, string contact spanners, and so forth. Bow contact spanners and
string contact spanners create conflicting typographic overrides when
formatted, so it is necessary to duplicate the bowing voice multiple times,
maintaining only the relevant indicators and spanners in each duplicate. By
subclassing Consort's built-in segment-maker \emph{Armilla} can extend its
score post-processing with an additional step consisting of these voice copying
operations.

Last, the segment-maker wraps the score inside a \texttt{LilyPondFile} instance
-- Abjad's object model for LilyPond input files -- and configures the LilyPond
file with any necessary stylesheet file includes. By subclassing Consort's
segment-maker, individual score packages can teach their segment-maker
subclasses to automatically locate their stylesheet files relative to the
package where that segment-maker subclass was defined. Such stylesheets should
include context definitions for the \texttt{TimeSignatureContext} as well as
any other custom contexts defined for the score. As a final test, the
segment-maker runs Abjad's battery of component well-formedness checks against
the score. These checks look for score-structure errors such as components
appearing in multiple places in the score, ties which join notes with differing
pitches, overlapping glissandi, or components with null parent references. If
all tests pass, the segment-maker returns the LilyPond file object.
Interpretation is complete.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Persistence \& visualization}
\label{sec:persistence-and-visualization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Once interpreted, a segment-maker's illustration may be persisted to disk as
LilyPond syntax for inclusion in other LilyPond files, rendered as a PDF for
viewing, or even serialized for other purposes. Composers study the results of
interpretation, make changes to each segment's specification, and re-interpret
as necessary, a large-scale re-enactment of interactive programming's pervasive
\emph{read-eval-print} loop paradigm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Conclusion}
%\label{sec:a-model-of-notation-conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\todo[inline]{\textbf{TODO:} Summarize this chapter for conclusion.}